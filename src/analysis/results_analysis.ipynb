{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notbeook performs the inference and analysis for a chosen experiment.\n",
    "\n",
    "**Instructions**\n",
    "* In the second cell you may change the path to the results folders. The default values should work, if when creating the worlds and running the experiment left the default results paths values.\n",
    "* In the section choose experiment for analysis, you can opt which experiment to run analysis for by uncommeting the related info with comments. To rerun analysis for another experiment, restart notebook, uncomment only the experiment to run analysis for.\n",
    "* with_spatial_flip variable indicates wether the SpatialFlip method shouls be included for the analysis. It is set to true only for statistical parity experiments and for experiments where ran withiing reasonable time. You can set it to false to run analysis without it\n",
    "* PROMIS methods with apply_fit_flips=true, and SpatialFlip method in inference just apply the precomputed flips.\n",
    "\n",
    "**Analysis**\n",
    "1. Reads related experiment info data.\n",
    "2. Reads pretrained models for SpatialFlip, PROMIS methods, performs predictions for test set and reads precomputed predictions for FairWhere method.\n",
    "3. Computes MLR (for statistical parity or equal opportunity depending on the experiment).\n",
    "4. Accuracy/F1 score, except for LAR (LAR includes only predictions), unfair by design (is synthetic) experiment.\n",
    "5. Disparity (FairWhere unfairness score definition) only for the DNN experiment.\n",
    "6. Computes normalized statistics (LR) (by dividing with the maximum statistic of the initial world).\n",
    "7. Show the above computed metrics plus fit times, budgets where PROMIS Opt reached limit, final budget metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "from analysis.analyse_results_func import *\n",
    "from sklearn import metrics\n",
    "from utils.plot_utils import *\n",
    "from utils.data_utils import (\n",
    "    read_scanned_regs,\n",
    "    get_y,\n",
    "    get_pos_info_regions,\n",
    "    read_all_models,\n",
    ")\n",
    "from utils.scores import get_mlr\n",
    "from utils.results_names_utils import get_train_val_test_paths, combine_world_info\n",
    "from sklearn import metrics\n",
    "import ast\n",
    "import random\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/\"\n",
    "results_base_path = \"../../results/\" # path to the base results folder\n",
    "save_plots_base_path = \"../../plots/\" # path to the base plots folder to save the plots or \"\" to not save them \n",
    "\n",
    "with_spatial_flip = False\n",
    "apply_fit_flips = False\n",
    "only_methods = []  # indicate all methods to be used for the analysis\n",
    "\n",
    "dnn_exp_dir = \"dnn_exp/\" # directory name for the DNN experiments\n",
    "xgb_eq_opp_dir = \"xgb_eq_opp_exp/\" # directory name for the XGB experiments with equal opportunity fairness notion\n",
    "lar_exp_dir = \"lar_exp/\" # directory name for the LAR experiments\n",
    "semi_synth_dir = \"crime_semi_synth_exp/\" # directory name for the semi synthetic experiments\n",
    "dataset_name = \"crime\" # default dataset name. For the LAR dataset, it is set to \"lar\"\n",
    "lar_dataset_name = \"lar\"\n",
    "\n",
    "figsize = (20, 8) \n",
    "display_title = True\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)  \n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Experiment for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Experiment (Equal Opportunity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Experiment (Statistical Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAR Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, with_spatial_flip, apply_fit_flips = (\n",
    "#     \"\",\n",
    "#     \"non_overlap_k_100\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, apply_fit_flips = (\n",
    "#     \"\",\n",
    "#     \"overlap_k_100_radii_30\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, with_spatial_flip, apply_fit_flips, with_spatial_flip = (\n",
    "#     \"\",\n",
    "#     \"5_x_5\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     False, \n",
    "#     True,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi Synthetic Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_non_overlap_k_8\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True,\n",
    "# )\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_5_x_5\",\n",
    "#     \"5_x_5\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True\n",
    "# )\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_overlap_k_10_radii_4\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Experiment (Equal Opportunity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"xgb\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "# )\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"xgb\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     False,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "# )\n",
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"xgb\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_path = os.path.join(results_base_path, dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Display Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_to_display_name = {\n",
    "    \"iter\": \"Spatial Flip\",\n",
    "    \"promis_app\": \"PROMIS App\",\n",
    "    \"promis_opt\": \"PROMIS Opt\",\n",
    "    \"promis_opt_wlimit_300\": \"PROMIS Opt\",\n",
    "    \"promis_opt_wlimit_1800\": \"PROMIS Opt (wlimit=1800)\",\n",
    "    \"promis_opt_wlimit_3600\": \"PROMIS Opt (wlimit=3600)\",\n",
    "    \"init\": \"Initial World\",\n",
    "}\n",
    "\n",
    "colors_list = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"black\",\n",
    "    \"darkred\",\n",
    "    \"darkgreen\",\n",
    "    \"darkblue\",\n",
    "    \"darkmagenta\",\n",
    "    \"darkcyan\",\n",
    "    \"darkorange\",\n",
    "    \"darkviolet\",\n",
    "    \"darkturquoise\",\n",
    "    \"darkslategray\",\n",
    "    \"darkgoldenrod\",\n",
    "    \"darkolivegreen\",\n",
    "    \"darkseagreen\",\n",
    "    \"darkslateblue\",\n",
    "    \"darkkhaki\",\n",
    "]\n",
    "\n",
    "\n",
    "method_to_plot_info = {\n",
    "    \"promis_app\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"darkgreen\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"promis_opt\": {\"linewidth\": 6, \"color\": \"black\", \"linestyle\": \"-\", \"scatter_marker\": \"o\"},\n",
    "    \"promis_opt_wlimit_300\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"promis_opt_wlimit_3600\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"purple\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"promis_opt_wlimit_1800\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"purple\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"iter\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"saddlebrown\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"where\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"blue\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    },\n",
    "    \"init\": {\n",
    "        \"linewidth\": 6,\n",
    "        \"color\": \"darkorange\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 100,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Trained Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    dataset_name, partioning_type_name, clf_name\n",
    ")\n",
    "train_path_info, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, dataset_name\n",
    ")\n",
    "\n",
    "(\n",
    "    val_regions_df,\n",
    "    val_pred_df,\n",
    "    val_labels_df,\n",
    "    y_pred_val,\n",
    "    y_pred_probs_val,\n",
    "    y_true_val,\n",
    "    val_points_per_region,\n",
    "    pos_y_true_indices_val,\n",
    "    pos_points_per_region_val,\n",
    ") = (\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "if dataset_name == \"lar\":\n",
    "    test_regions_df = read_scanned_regs(train_path_info[\"regions\"])\n",
    "    test_pred_df = pd.read_csv(f\"{base_path}preprocess/lar.csv\")\n",
    "    y_pred_test = get_y(test_pred_df, \"label\")\n",
    "    y_true_test = None\n",
    "    y_pred_probs_test=None\n",
    "\n",
    "    test_points_per_region = test_regions_df[\"points\"].tolist()\n",
    "else:\n",
    "    test_regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "    test_pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "    test_labels_df = pd.read_csv(test_path_info[\"labels\"])\n",
    "    y_pred_test = get_y(test_pred_df, \"pred\")\n",
    "    y_pred_probs_test = get_y(test_pred_df, \"prob\") if not clf_name.startswith(\"semi_synthetic\") else None\n",
    "    y_true_test = get_y(test_labels_df, \"label\")\n",
    "\n",
    "    test_points_per_region = test_regions_df[\"points\"].tolist()\n",
    "\n",
    "\n",
    "if dataset_name != \"lar\":\n",
    "    pos_y_true_indices_test, pos_points_per_region_test = get_pos_info_regions(\n",
    "        y_true_test, test_points_per_region\n",
    "    )\n",
    "else:\n",
    "    pos_y_true_indices_test, pos_points_per_region_test = None, None\n",
    "\n",
    "results_path = f\"{results_base_path}{res_desc_label}/\"\n",
    "\n",
    "if save_plots_base_path:\n",
    "    save_plots_path = os.path.join(save_plots_base_path, dir_name, res_desc_label, f\"{fairness_notion}/\")\n",
    "    print(f\"Save plots path: {save_plots_path}\")\n",
    "    os.makedirs(save_plots_path, exist_ok=True)\n",
    "\n",
    "sp_flip_meths_2_pretrained_models = {}\n",
    "if with_spatial_flip:\n",
    "    sp_flip_meths_2_pretrained_models = read_all_models(\n",
    "        f\"{results_path}spatial_flip_models/{fairness_notion}/\",\n",
    "        False,\n",
    "        methods=only_methods,\n",
    "    )\n",
    "\n",
    "sp_opt_meths_2_pretrained_models = read_all_models(\n",
    "    f\"{results_path}spatial_optim_models/{fairness_notion}/\", True, methods=only_methods\n",
    ")\n",
    "all_meths_2_pretrained_models = {\n",
    "    **sp_flip_meths_2_pretrained_models,\n",
    "    **sp_opt_meths_2_pretrained_models,\n",
    "}\n",
    "\n",
    "splitted_labels = get_all_methods_modes_labels(\n",
    "    list(all_meths_2_pretrained_models.keys())\n",
    ")\n",
    "\n",
    "opt_methods_display_labels = splitted_labels[\"opt_labels\"]\n",
    "for label in all_meths_2_pretrained_models.keys():\n",
    "    if label not in method_to_display_name:\n",
    "        method_to_display_name[label] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Predictions - Compute Results Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_methods_to_results_info, budget_range = compute_all_results_info(\n",
    "    all_meths_2_pretrained_models=all_meths_2_pretrained_models,\n",
    "    test_points_per_region=test_points_per_region,\n",
    "    y_pred_test_probs=y_pred_probs_test,\n",
    "    y_true_test=y_true_test,\n",
    "    y_pred_test_orig=y_pred_test,\n",
    "    apply_fit_flips=apply_fit_flips,\n",
    ")\n",
    "\n",
    "sp_flip_methods_2_results_info = {\n",
    "    k: v\n",
    "    for k, v in all_methods_to_results_info.items()\n",
    "    if k in splitted_labels[\"heu_labels\"]\n",
    "}\n",
    "sp_opt_methods_2_results_info = {\n",
    "    k: v\n",
    "    for k, v in all_methods_to_results_info.items()\n",
    "    if k in splitted_labels[\"opt_labels\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Disparity, Metrics on FairWhere Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    P_where_test,\n",
    "    N_where_test,\n",
    "    RHO_where_test,\n",
    "    TP_where_test,\n",
    "    TPR_where_test,\n",
    "    mlr_where_test_st_par,\n",
    "    mlr_where_test_eq_opp,\n",
    "    acc_where_test,\n",
    "    f1_where_test,\n",
    "    where_fit_time,\n",
    "    where_fair_score_test,\n",
    "    init_fair_score_test,\n",
    "    where_fairness_loss_sum_test,\n",
    "    init_fairness_loss_sum_test,\n",
    "    where_fairness_loss_weighted_sum_test,\n",
    "    init_fairness_loss_sum_weighted_test,\n",
    ") = (\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "\n",
    "\n",
    "def get_pr(y_pred):\n",
    "    if len(y_pred) == 0:\n",
    "        return 0\n",
    "    return np.sum(y_pred) / len(y_pred)\n",
    "\n",
    "\n",
    "if clf_name.startswith(\"dnn\"):\n",
    "    if fairness_notion == \"equal_opportunity\":\n",
    "        fair_score_func = metrics.recall_score\n",
    "    else:\n",
    "        fair_score_func = get_pr\n",
    "\n",
    "    with open(f\"{results_path}{dataset_name}_{fairness_notion}_where_fit_time.txt\", \"r\") as file:\n",
    "        where_fit_time = float(file.read())\n",
    "\n",
    "\n",
    "    where_pred_test_df = pd.read_csv(\n",
    "        f\"{results_path}{dataset_name}_{fairness_notion}_where_model_test_pred.csv\"\n",
    "    )\n",
    "    test_partitioning_id_df = pd.read_csv(\n",
    "        f\"{base_path}partitionings/test_{partioning_name}_partitioning_ids.csv\"\n",
    "    )\n",
    "\n",
    "    y_pred_where_test = get_y(where_pred_test_df, \"pred\")\n",
    "\n",
    "    test_partitioning_id_df[\"id\"] = test_partitioning_id_df[\"id\"].apply(\n",
    "        ast.literal_eval\n",
    "    )\n",
    "    test_partitioning_id_df[\"partitioning\"] = test_partitioning_id_df[\n",
    "        \"partitioning\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    test_ids = test_partitioning_id_df[\"id\"].tolist()\n",
    "    test_partitionings = test_partitioning_id_df[\"partitioning\"].tolist()\n",
    "\n",
    "    P_where_test = np.sum(y_pred_where_test)\n",
    "    N_where_test = len(y_pred_where_test)\n",
    "    RHO_where_test = P_where_test / N_where_test\n",
    "    TP_where_test = np.sum(y_pred_where_test[pos_y_true_indices_test])\n",
    "    TPR_where_test = TP_where_test / len(pos_y_true_indices_test)\n",
    "\n",
    "    # MLR\n",
    "    mlr_where_test_st_par = get_mlr(y_pred_where_test, test_points_per_region)\n",
    "    mlr_where_test_eq_opp = get_mlr(\n",
    "        y_pred_where_test[pos_y_true_indices_test], pos_points_per_region_test\n",
    "    )\n",
    "\n",
    "    # Accuracy\n",
    "    acc_where_test = metrics.accuracy_score(y_true_test, y_pred_where_test)\n",
    "    f1_where_test = metrics.f1_score(y_true_test, y_pred_where_test)\n",
    "\n",
    "    where_fairness_loss_list_test = get_partionings_fairness_loss_all(\n",
    "        y_pred_where_test,\n",
    "        test_ids,\n",
    "        test_partitionings,\n",
    "        y_true_test,\n",
    "        weighted=False,\n",
    "        score_func=fair_score_func,\n",
    "    )\n",
    "\n",
    "    where_fairness_loss_sum_test = np.sum(where_fairness_loss_list_test)\n",
    "\n",
    "    init_fairness_loss_list_test = get_partionings_fairness_loss_all(\n",
    "        y_pred_test,\n",
    "        test_ids,\n",
    "        test_partitionings,\n",
    "        y_true_test,\n",
    "        weighted=False,\n",
    "        score_func=fair_score_func,\n",
    "    )\n",
    "\n",
    "    init_fairness_loss_sum_test = np.sum(init_fairness_loss_list_test)\n",
    "\n",
    "    # fair loss score weighed per partitioning\n",
    "    where_fairness_loss_list_weighted_test = get_partionings_fairness_loss_all(\n",
    "        y_pred_where_test,\n",
    "        test_ids,\n",
    "        test_partitionings,\n",
    "        y_true_test,\n",
    "        weighted=True,\n",
    "        score_func=fair_score_func,\n",
    "    )\n",
    "\n",
    "    where_fairness_loss_weighted_sum_test = np.sum(\n",
    "        where_fairness_loss_list_weighted_test\n",
    "    )\n",
    "\n",
    "    init_fairness_loss_list_weighted_test = get_partionings_fairness_loss_all(\n",
    "        y_pred_test,\n",
    "        test_ids,\n",
    "        test_partitionings,\n",
    "        y_true_test,\n",
    "        weighted=True,\n",
    "        score_func=fair_score_func,\n",
    "    )\n",
    "\n",
    "    init_fairness_loss_sum_weighted_test = np.sum(init_fairness_loss_list_weighted_test)\n",
    "\n",
    "    for method, res_df in all_methods_to_results_info.items():\n",
    "        res_df[\"fair_loss_list_test\"] = res_df[\"y_pred_test\"].apply(\n",
    "            lambda x: get_partionings_fairness_loss_all(\n",
    "                x,\n",
    "                test_ids,\n",
    "                test_partitionings,\n",
    "                y_true_test,\n",
    "                weighted=False,\n",
    "                score_func=fair_score_func,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        res_df[\"fair_loss_sum_test\"] = res_df[\"fair_loss_list_test\"].apply(\n",
    "            lambda x: np.sum(x)\n",
    "        )\n",
    "\n",
    "        res_df[\"fair_loss_list_weighted_test\"] = res_df[\"y_pred_test\"].apply(\n",
    "            lambda x: get_partionings_fairness_loss_all(\n",
    "                x,\n",
    "                test_ids,\n",
    "                test_partitionings,\n",
    "                y_true_test,\n",
    "                weighted=True,\n",
    "                score_func=fair_score_func,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        res_df[\"fair_loss_sum_weighted_test\"] = res_df[\n",
    "            \"fair_loss_list_weighted_test\"\n",
    "        ].apply(lambda x: np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_name.startswith(\"dnn\"):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 6))  \n",
    "\n",
    "    test_ids_str = [str(i) for i in test_ids]\n",
    "\n",
    "    # Non-weighted Fairness Loss\n",
    "\n",
    "    axes[0].plot(test_ids_str, init_fairness_loss_list_test, label=\"base\")\n",
    "    axes[0].scatter(test_ids_str, init_fairness_loss_list_test)\n",
    "    axes[0].plot(\n",
    "        test_ids_str, where_fairness_loss_list_test, label=\"where\", linestyle=\"dashed\"\n",
    "    )\n",
    "    axes[0].scatter(test_ids_str, where_fairness_loss_list_test)\n",
    "    axes[0].set_xlabel(\"Partitioning Id\")\n",
    "    axes[0].set_ylabel(\"Fairness Loss\")\n",
    "    if display_title:\n",
    "        axes[0].set_title(\"Fairness Loss per Partitioning (Test)\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Weighted Fairness Loss\n",
    "    axes[1].plot(test_ids_str, init_fairness_loss_list_weighted_test, label=\"base\")\n",
    "    axes[1].scatter(test_ids_str, init_fairness_loss_list_weighted_test)\n",
    "    axes[1].plot(\n",
    "        test_ids_str,\n",
    "        where_fairness_loss_list_weighted_test,\n",
    "        label=\"where\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    axes[1].scatter(test_ids_str, where_fairness_loss_list_weighted_test)\n",
    "    axes[1].set_xlabel(\"Partitioning Id\")\n",
    "    axes[1].set_ylabel(\"Weighted Fairness Loss\")\n",
    "    if display_title:\n",
    "        axes[1].set_title(\"Weighted Fairness Loss per Partitioning (Test)\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()  \n",
    "\n",
    "    if save_plots_base_path:\n",
    "        plt.savefig(\n",
    "            f\"{save_plots_path}fairness_loss_per_partitioning.pdf\", format=\"pdf\"\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_labels = []\n",
    "fair_base_labels = [\"mlr\", \"fair_mlr_ratio\"]\n",
    "if fairness_notion == \"equal_opportunity\":\n",
    "    for fair_base_label in fair_base_labels:\n",
    "        fair_labels.append(f\"{fair_base_label}_st_par\")\n",
    "\n",
    "if fairness_notion == \"equal_opportunity\":\n",
    "    for fair_base_label in fair_base_labels:\n",
    "        fair_labels.append(f\"{fair_base_label}_eq_opp\")\n",
    "\n",
    "P_test = np.sum(y_pred_test)\n",
    "N_test = len(y_pred_test)\n",
    "RHO_test = P_test / N_test\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"N_test: {N_test}\")\n",
    "print(f\"P_test: {P_test}\")\n",
    "print(f\"RHO_test: {RHO_test:.3f}\")\n",
    "\n",
    "if dataset_name != \"lar\":\n",
    "    TP_test = np.sum(y_pred_test[pos_y_true_indices_test])\n",
    "    TPR_test = TP_test / len(pos_y_true_indices_test)\n",
    "    print(f\"TPR_test: {TPR_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "signif_level = 0.005\n",
    "n_alt_worlds = 200\n",
    "\n",
    "\n",
    "def to_region_dict(pts_per_region):\n",
    "    return [{\"points\": reg} for reg in pts_per_region]\n",
    "\n",
    "\n",
    "init_test_mlr_st_par, init_test_stats_st_par = get_mlr(\n",
    "    y_pred_test, test_points_per_region, with_stats=True\n",
    ")\n",
    "\n",
    "init_test_mlr_eq_opp, init_test_stats_eq_opp = get_mlr(\n",
    "    y_pred_test[pos_y_true_indices_test], pos_points_per_region_test, with_stats=True\n",
    ") if dataset_name != \"lar\" else (None, None)\n",
    "\n",
    "init_acc_test = metrics.accuracy_score(y_true_test, y_pred_test) if y_true_test is not None else None\n",
    "init_f1_test = metrics.f1_score(y_true_test, y_pred_test) if y_true_test is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Budgets where Optimization Reach Limit (i.e. PROMIS Opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    if \"status\" in all_methods_to_results_info[method].columns:\n",
    "        unique_status = list(all_methods_to_results_info[method][\"status\"].unique())\n",
    "        for status in unique_status:\n",
    "            if status not in [1, 3]:\n",
    "                other_status_exp_idxs = all_methods_to_results_info[method][\n",
    "                    all_methods_to_results_info[method][\"status\"] == status\n",
    "                ][\"exp_idx\"].unique()\n",
    "                print(\n",
    "                    f\"Found status {status} for method {method} for exp indexes: {other_status_exp_idxs}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_tlimit_cnt = {\n",
    "    method: 0\n",
    "    for method in splitted_labels[\"opt_labels\"]\n",
    "    if \"status\" in all_methods_to_results_info[method].columns\n",
    "}\n",
    "\n",
    "labels = []\n",
    "status_lists = []\n",
    "budget_lists = []\n",
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    if \"status\" in all_methods_to_results_info[method].columns:\n",
    "        labels.append(method)\n",
    "        status_list = all_methods_to_results_info[method][\"status\"].to_list()\n",
    "        budget_list = all_methods_to_results_info[method][\"budget\"].to_list()\n",
    "        tlimit_cnt = len(np.where(np.array(status_list) == 3)[0])\n",
    "        method_tlimit_cnt[method] += tlimit_cnt\n",
    "\n",
    "        status_lists.append(status_list)\n",
    "        budget_lists.append(budget_list)\n",
    "\n",
    "plot_opt_methods_status(\n",
    "    labels=labels,\n",
    "    budget_lists=budget_lists,\n",
    "    status_lists=status_lists,\n",
    "    save_path=save_plots_path,\n",
    "    figsize=figsize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meths_min_C_reach_limit = {}\n",
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    res_df = all_methods_to_results_info[method]\n",
    "    if 3 in res_df.status.tolist():\n",
    "        meths_min_C_reach_limit[method] = res_df[res_df[\"status\"] == 3][\"budget\"].min()\n",
    "\n",
    "\n",
    "plot_min_C_reach_limit(\n",
    "    meths_min_C_reach_limit,\n",
    "    method_to_plot_info,\n",
    "    method_to_display_name,\n",
    "    opt_methods_display_labels,\n",
    "    figsize=figsize,\n",
    "    save_path=save_plots_path,\n",
    "    display_title=display_title,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flips_limit = None\n",
    "if fairness_notion == \"statistical_parity\":\n",
    "    plot_scores(\n",
    "        all_methods_to_results_info,\n",
    "        init_test_mlr_st_par,\n",
    "        method_to_plot_info,\n",
    "        method_to_display_name,\n",
    "        opt_methods_display_labels,\n",
    "        save_plots_path,\n",
    "        figsize=figsize,\n",
    "        flips_limit=flips_limit,\n",
    "        append_to_title=\" (Statistical Parity - Test Set)\",\n",
    "        append_to_save=\"_st_par_test\",\n",
    "        score_label=\"mlr_st_par_test\",\n",
    "        display_title=display_title,\n",
    "        axhline_mlr=mlr_where_test_st_par,\n",
    "        axhline_mlr_label=\"Where MLR\",\n",
    "    )\n",
    "\n",
    "if fairness_notion == \"equal_opportunity\":\n",
    "    plot_scores(\n",
    "        all_methods_to_results_info,\n",
    "        init_test_mlr_eq_opp,\n",
    "        method_to_plot_info,\n",
    "        method_to_display_name,\n",
    "        opt_methods_display_labels,\n",
    "        save_plots_path,\n",
    "        figsize=figsize,\n",
    "        flips_limit=flips_limit,\n",
    "        append_to_title=\" (Equal Opportunity - Test Set)\",\n",
    "        append_to_save=\"_eq_opp_test\",\n",
    "        score_label=\"mlr_eq_opp_test\",\n",
    "        display_title=display_title,\n",
    "        axhline_mlr=mlr_where_test_eq_opp,\n",
    "        axhline_mlr_label=\"Where\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- params[['mlr', 'pos_mlr', 'test_mlr', 'pos_test_mlr']]\n",
    "all_methods_to_results_info['cont_in_ov_over_eq_opp'][['sol_mlr', 'pos_mlr', 'new_val_mlr', 'val_new_pos_mlr', 'new_test_mlr', 'test_new_pos_mlr', 'budget']] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fairness_notion == \"statistical_parity\":\n",
    "    plot_flips_time(\n",
    "        all_methods_to_results_info,\n",
    "        method_to_display_name,\n",
    "        method_to_plot_info,\n",
    "        opt_methods_display_labels,\n",
    "        title_append=\" (Statistical Parity)\",\n",
    "        save_append=\"_st_par\",\n",
    "        save_plots_path=save_plots_path,\n",
    "        log_time=False,\n",
    "        figsize=figsize,\n",
    "        display_title=display_title,\n",
    "        axhline_time=where_fit_time,\n",
    "        axhline_time_label=\"Where\",\n",
    "    )\n",
    "    plot_flips_time(\n",
    "        all_methods_to_results_info,\n",
    "        method_to_display_name,\n",
    "        method_to_plot_info,\n",
    "        opt_methods_display_labels,\n",
    "        title_append=\" (Statistical Parity)\",\n",
    "        save_append=\"_st_par\",\n",
    "        save_plots_path=save_plots_path,\n",
    "        log_time=True,\n",
    "        figsize=figsize,\n",
    "        display_title=display_title,\n",
    "        axhline_time=where_fit_time,\n",
    "        axhline_time_label=\"Where\",\n",
    "    )\n",
    "\n",
    "if fairness_notion == \"equal_opportunity\":\n",
    "    plot_flips_time(\n",
    "        all_methods_to_results_info,\n",
    "        method_to_display_name,\n",
    "        method_to_plot_info,\n",
    "        opt_methods_display_labels,\n",
    "        title_append=\"(Equal Opportunity)\",\n",
    "        save_append=\"_eq_opp\",\n",
    "        save_plots_path=save_plots_path,\n",
    "        log_time=False,\n",
    "        figsize=figsize,\n",
    "        display_title=display_title,\n",
    "        axhline_time=where_fit_time,\n",
    "        axhline_time_label=\"Where\",\n",
    "    )\n",
    "    plot_flips_time(\n",
    "        all_methods_to_results_info,\n",
    "        method_to_display_name,\n",
    "        method_to_plot_info,\n",
    "        opt_methods_display_labels,\n",
    "        title_append=\"(Equal Opportunity)\",\n",
    "        save_append=\"_eq_opp\",\n",
    "        save_plots_path=save_plots_path,\n",
    "        log_time=True,\n",
    "        figsize=figsize,\n",
    "        display_title=display_title,\n",
    "        axhline_time=where_fit_time,\n",
    "        axhline_time_label=\"Where\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes Maximum Budget Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"init\"]\n",
    "final_mlrs_st_par_test = [init_test_mlr_st_par]\n",
    "final_mlrs_eq_opp_test = [init_test_mlr_eq_opp]\n",
    "final_times = [None]\n",
    "n_flips_ = budget_range[-1]\n",
    "budget_list = [0] + [n_flips_] * len(all_methods_to_results_info)\n",
    "final_stats_st_par_test_list = [init_test_stats_st_par]\n",
    "final_stats_eq_opp_test_list = [init_test_stats_eq_opp]\n",
    "final_performance_label = \"f1\" if clf_name.startswith(\"dnn\") else \"accuracy\"\n",
    "final_performance_test_list = [init_f1_test] if clf_name.startswith(\"dnn\") else [init_acc_test]\n",
    "final_fair_score_test_list = [init_fairness_loss_sum_test]\n",
    "for method, exp_res_df in all_methods_to_results_info.items():\n",
    "    if fairness_notion == \"statistical_parity\":\n",
    "        mlr_st_par_test = exp_res_df[exp_res_df[\"budget\"] == n_flips_][\n",
    "            \"mlr_st_par_test\"\n",
    "        ].tolist()[0]\n",
    "        final_mlrs_st_par_test.append(mlr_st_par_test)\n",
    "        y_test_pred = exp_res_df[exp_res_df[\"budget\"] == n_flips_][\n",
    "            \"y_pred_test\"\n",
    "        ].tolist()[0]\n",
    "        _, final_stats_st_par_test = get_mlr(\n",
    "            y_test_pred, test_points_per_region, with_stats=True\n",
    "        )\n",
    "        final_stats_st_par_test_list.append(final_stats_st_par_test)\n",
    "        if clf_name.startswith(\"dnn\"):\n",
    "            final_fair_score_test_list.append(\n",
    "                exp_res_df[exp_res_df[\"budget\"] == n_flips_][\"fair_loss_sum_test\"].tolist()[0]\n",
    "            )\n",
    "    else:\n",
    "        mlr_eq_opp_test = exp_res_df[exp_res_df[\"budget\"] == n_flips_][\n",
    "            \"mlr_eq_opp_test\"\n",
    "        ].tolist()[0]\n",
    "        y_test_pred = exp_res_df[exp_res_df[\"budget\"] == n_flips_][\n",
    "            \"y_pred_test\"\n",
    "        ].tolist()[0]\n",
    "        y_test_pred_pos = y_test_pred[pos_y_true_indices_test]\n",
    "        _, final_stats_eq_opp_test = get_mlr(\n",
    "            y_test_pred_pos, pos_points_per_region_test, with_stats=True\n",
    "        )\n",
    "        final_stats_eq_opp_test_list.append(final_stats_eq_opp_test)\n",
    "\n",
    "        if clf_name.startswith(\"dnn\"):\n",
    "            final_fair_score_test_list.append(\n",
    "                exp_res_df[exp_res_df[\"budget\"] == n_flips_][\"fair_loss_sum_test\"].tolist()[0]\n",
    "            )\n",
    "\n",
    "    final_flip_time = exp_res_df[exp_res_df[\"budget\"] == n_flips_][\"time\"].tolist()[0]\n",
    "    final_times.append(final_flip_time)\n",
    "    methods.append(method)\n",
    "    if y_true_test is not None:\n",
    "        final_performance_test_list.append(\n",
    "            exp_res_df[exp_res_df[\"budget\"] == n_flips_][f\"{final_performance_label}_test\"].tolist()[0]\n",
    "        )\n",
    "    else:\n",
    "        final_performance_test_list.append(None)\n",
    "        \n",
    "    if fairness_notion == \"equal_opportunity\":\n",
    "        final_mlrs_eq_opp_test.append(mlr_eq_opp_test)\n",
    "\n",
    "final_results = {\n",
    "    \"budget\": budget_list,\n",
    "    \"Method\": methods,\n",
    "    \"time\": final_times,\n",
    "    f\"{final_performance_label}_test\": final_performance_test_list,\n",
    "}\n",
    "if fairness_notion == \"statistical_parity\":\n",
    "    final_results[\"mlr_st_par_test\"] = final_mlrs_st_par_test\n",
    "    final_results[\"final_stats_st_par_test\"] = final_stats_st_par_test_list\n",
    "else:\n",
    "    final_results[\"mlr_eq_opp_test\"] = final_mlrs_eq_opp_test\n",
    "    final_results[\"final_stats_eq_opp_test\"] = final_stats_eq_opp_test_list\n",
    "\n",
    "if clf_name.startswith(\"dnn\"):\n",
    "    final_results[\"Method\"].append(\"Where\")\n",
    "    final_results[\"budget\"].append(n_flips_)\n",
    "    final_results[\"time\"].append(where_fit_time)\n",
    "    final_results[\"f1_test\"].append(f1_where_test)\n",
    "    if fairness_notion == \"statistical_parity\":\n",
    "        final_results[\"mlr_st_par_test\"].append(mlr_where_test_st_par)\n",
    "        _, final_stats_st_par_test = get_mlr(\n",
    "            y_pred_where_test, test_points_per_region, with_stats=True\n",
    "        )\n",
    "        final_results[\"final_stats_st_par_test\"].append(final_stats_st_par_test)\n",
    "        final_fair_score_test_list.append(where_fairness_loss_sum_test)\n",
    "        final_results['Fair Loss Sum (Test)'] = final_fair_score_test_list\n",
    "\n",
    "    else:\n",
    "        final_results[\"mlr_eq_opp_test\"].append(mlr_where_test_eq_opp)\n",
    "        _, final_stats_eq_opp_test = get_mlr(\n",
    "            y_pred_where_test[pos_y_true_indices_test],\n",
    "            pos_points_per_region_test,\n",
    "            with_stats=True,\n",
    "        )\n",
    "        final_results[\"final_stats_eq_opp_test\"].append(final_stats_eq_opp_test)\n",
    "        final_fair_score_test_list.append(where_fairness_loss_sum_test)\n",
    "        final_results['Fair Loss Sum (Test)'] = final_fair_score_test_list\n",
    "\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "if save_plots_base_path:\n",
    "    final_results_df.to_csv(f\"{save_plots_path}final_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MLR/Mean Disparity vs Accuracy/F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scores = {\n",
    "    \"mlr_st_par\": {\n",
    "        \"test\": init_test_mlr_st_par,\n",
    "    },\n",
    "    \"mlr_eq_opp\": {\n",
    "        \"test\": init_test_mlr_eq_opp,\n",
    "    },\n",
    "    \"accuracy\": {\n",
    "        \"test\": init_acc_test,\n",
    "    },\n",
    "    \"f1\": {\n",
    "        \"test\": init_f1_test,\n",
    "    },\n",
    "    \"fair_loss_sum\":\n",
    "    {\n",
    "        \"test\": init_fairness_loss_sum_test,\n",
    "    }\n",
    "}\n",
    "\n",
    "where_scores = {\n",
    "    \"mlr_st_par\": {\n",
    "        \"test\": mlr_where_test_st_par,\n",
    "    },\n",
    "    \"mlr_eq_opp\": {\n",
    "        \"test\": mlr_where_test_eq_opp,\n",
    "    },\n",
    "    \"accuracy\": {\n",
    "        \"test\": acc_where_test,\n",
    "    },\n",
    "    \"f1\": {\n",
    "        \"test\": f1_where_test,\n",
    "    },\n",
    "    \"fair_loss_sum\":\n",
    "    {\n",
    "        \"test\": where_fairness_loss_sum_test,\n",
    "    }\n",
    "}\n",
    "fair_scores_display_labels = {\n",
    "    \"mlr_st_par\": \"MLR\",\n",
    "    \"mlr_eq_opp\": \"MLR\",\n",
    "    \"fair_loss_sum\": \"Mean Disparity\"\n",
    "}\n",
    "performance_scores_display_labels = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"f1\": \"F1 Score\",\n",
    "}\n",
    "sets_display_labels = {\n",
    "    \"sol\": \"Solution\",\n",
    "    \"val\": \"Validation Set\",\n",
    "    \"test\": \"Test Set\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1_vs_score2_figsize = (14, 8) if display_title else figsize\n",
    "\n",
    "if y_true_test is not None:\n",
    "    sets = [\"test\"]\n",
    "    if fairness_notion == \"statistical_parity\":\n",
    "        fair_scores = [\"mlr_st_par\"]\n",
    "    else:\n",
    "        fair_scores = [\"mlr_eq_opp\"]\n",
    "\n",
    "    if clf_name.startswith(\"dnn\"):\n",
    "        fair_scores.append(\"fair_loss_sum\")\n",
    "        performance_scores = [\"f1\"]\n",
    "    else:\n",
    "        performance_scores = [\"accuracy\"]\n",
    "\n",
    "    for set_ in sets:\n",
    "        for fair_score in fair_scores:\n",
    "            for performance_score in performance_scores:\n",
    "                plot_score1_vs_score2(\n",
    "                    methods_to_res_info=all_methods_to_results_info,\n",
    "                    score_label1=f\"{fair_score}_{set_}\",\n",
    "                    score_label2=f\"{performance_score}_{set_}\",\n",
    "                    score_display_label1=fair_scores_display_labels[fair_score],\n",
    "                    score_display_label2=performance_scores_display_labels[performance_score],\n",
    "                    init_score1=init_scores[fair_score][set_],\n",
    "                    init_score2=init_scores[performance_score][set_],\n",
    "                    method_to_plot_info=method_to_plot_info,\n",
    "                    method_to_display_name=method_to_display_name,\n",
    "                    opt_methods_display_labels=opt_methods_display_labels,\n",
    "                    save_plots_path=save_plots_path,\n",
    "                    figsize=score1_vs_score2_figsize,\n",
    "                    append_to_title=f\" ({sets_display_labels[set_]})\",\n",
    "                    append_to_save=f\"_{fair_score}_{set_}\",\n",
    "                    display_title=display_title,\n",
    "                    other_score1=where_scores[fair_score][set_],\n",
    "                    other_score2=where_scores[performance_score][set_],\n",
    "                    other_method_label=\"Where\",\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot P, RHO, Actual Flips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_methods_info(\n",
    "    all_methods_to_results_info,\n",
    "    P_test,\n",
    "    RHO_test,\n",
    "    p_label=\"P_test\",\n",
    "    rho_label=\"RHO_test\",\n",
    "    actual_flips_label=\"actual_flips_test\",\n",
    "    method_to_plot_info=method_to_plot_info,\n",
    "    method_to_display_name=method_to_display_name,\n",
    "    opt_methods_display_labels=opt_methods_display_labels,\n",
    "    save_path=save_plots_path,\n",
    "    figsize=figsize,\n",
    "    append_to_title=f\" ({fairness_notion} - Test Set)\",\n",
    "    display_title=display_title,\n",
    "    axhline_P=P_where_test,\n",
    "    axhline_RHO=RHO_where_test,\n",
    "    axhline_label=\"Where\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Normalized Statistics (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = \"Regions\"\n",
    "ylabel = \"Normalized LR\"\n",
    "\n",
    "methods_labels = final_results_df[\"Method\"].unique().tolist()\n",
    "if fairness_notion == \"statistical_parity\":\n",
    "    stats_per_method = (\n",
    "        final_results_df.groupby(\"Method\", sort=False)[\"final_stats_st_par_test\"]\n",
    "        .apply(list)\n",
    "        .tolist()\n",
    "    )\n",
    "    stats_per_method = [np.concatenate(stats).tolist() for stats in stats_per_method]\n",
    "    max_init_stat = max(init_test_stats_st_par)\n",
    "else:\n",
    "    stats_per_method = (\n",
    "        final_results_df.groupby(\"Method\", sort=False)[\"final_stats_eq_opp_test\"]\n",
    "        .apply(list)\n",
    "        .tolist()\n",
    "    )\n",
    "    stats_per_method = [np.concatenate(stats).tolist() for stats in stats_per_method]\n",
    "    max_init_stat = max(init_test_stats_eq_opp)\n",
    "\n",
    "# Plotting the combined barplot\n",
    "plot_regions_norm_stats(\n",
    "    methods_stats=stats_per_method,\n",
    "    methods_labels=methods_labels,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    max_stat=max_init_stat,\n",
    "    save_path=save_plots_path,\n",
    "    append_to_title=\"(Test Set)\",\n",
    "    display_title=display_title,\n",
    "    method_to_display_name=method_to_display_name,\n",
    "    method_to_plot_info=method_to_plot_info,\n",
    "    figsize=figsize,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promis_env_new",
   "language": "python",
   "name": "promis_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
