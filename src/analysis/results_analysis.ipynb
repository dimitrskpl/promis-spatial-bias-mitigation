{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notbeook performs the inference and analysis for a chosen experiment.\n",
    "\n",
    "**Instructions**\n",
    "* In the second cell you may change the path to the results folders. The default values should work, if the default results paths values were not changed.\n",
    "* In the section **Choose Experiment for Analysis**, you can opt which experiment to run analysis for by uncommeting the related commented info. To rerun analysis for another experiment, please restart the notebook and uncomment only the experiment to run analysis for.\n",
    "* with_spatial_flip variable indicates wether the SpatialFlip method should be included for the analysis. It is set to true only for statistical parity experiments. You can set it to false to run analysis without it.\n",
    "* apply_fit_flips=true indicates to PROMIS methods to apply the precomputed flips like SpatialFlip method does.\n",
    "* only_methods variable is a list indicating which methods to include in the analysis. It is used only in the experiment on the LAR dataset, to compare PROMIS Opt (wlimit=300) with PROMIS App and PROMIS opt (wlimit=300) with PROMIS opt (wlimit=1800). \n",
    "\n",
    "**Analysis**\n",
    "1. Reads related experiment info data.\n",
    "2. Reads pretrained models for SpatialFlip, PROMIS methods, performs predictions for test set and reads precomputed predictions for FairWhere method.\n",
    "3. Computes SBI (for statistical parity or equal opportunity depending on the experiment).\n",
    "4. Computes Accuracy/F1 score, except for LAR (which does not include ground truths), unfair by design (which is semi-synthetic) experiment.\n",
    "5. Computes MeanDev (FairWhere unfairness score definition) only for the DNN experiment.\n",
    "6. Shows the above computed metrics plus fit times, budgets where PROMIS Opt reached limit, final budget metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "from analysis.analyse_results_func import *\n",
    "from sklearn import metrics\n",
    "from utils.plot_utils import *\n",
    "from utils.data_utils import (\n",
    "    read_scanned_regs,\n",
    "    get_y,\n",
    "    get_pos_info_regions,\n",
    "    read_all_models,\n",
    ")\n",
    "from utils.scores import get_sbi\n",
    "from utils.results_names_utils import get_train_val_test_paths, combine_world_info\n",
    "from sklearn import metrics\n",
    "import ast\n",
    "import random\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/\"\n",
    "results_base_path = \"../../results/\" # path to the base results folder\n",
    "save_plots_base_path =  \"\" # path to the base plots folder to save the plots or \"\" to not save them \n",
    "final_budget_metrics_fname = \"final_budget_metrics.csv\" # file name to save the final budget results or \"\" to not save them \n",
    "\n",
    "with_spatial_flip = False # True: only for experiments with spatial flip to be considered\n",
    "apply_fit_flips = False # True: for all experiments where statistical parity is assessed except for the experiment with DNN model\n",
    "exact_wlim_comparison = False # True: compare only the PROMIS-Exact solutions computed with different working limits\n",
    "only_methods = []  # True: indicates which methods to be used for the analysis. If empty all methods are considered\n",
    "\n",
    "dnn_exp_dir = \"dnn_exp/\" # directory name for the DNN experiments\n",
    "xgb_eq_opp_dir = \"xgb_eq_opp_exp/\" # directory name for the XGB experiments with equal opportunity fairness notion\n",
    "lar_exp_dir = \"lar_exp/\" # directory name for the LAR experiments\n",
    "semi_synth_dir = \"crime_semi_synth_exp/\" # directory name for the semi synthetic experiments\n",
    "dataset_name = \"crime\" # default dataset name. For the LAR dataset, it is set to \"lar\"\n",
    "lar_dataset_name = \"lar\"\n",
    "\n",
    "figsize = (14, 8) \n",
    "display_title = True\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)  \n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Experiment for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Experiment (Equal Opportunity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name = (\n",
    "#     \"dnn\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     dnn_exp_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Experiment (Statistical Parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, with_spatial_flip = (\n",
    "#     \"dnn\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     dnn_exp_dir,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAR Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, apply_fit_flips, with_spatial_flip, only_methods = (\n",
    "#     \"\",\n",
    "#     \"non_overlap_k_100\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\", \"iter\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, apply_fit_flips, with_spatial_flip, only_methods = (\n",
    "#     \"\",\n",
    "#     \"5_x_5\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\", \"iter\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, apply_fit_flips, with_spatial_flip, only_methods = (\n",
    "#     \"\",\n",
    "#     \"overlap_k_100_radii_30\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\", \"iter\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAR Experiment - PROMIS Opt with With Several Work Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, with_spatial_flip, apply_fit_flips, only_methods, exact_wlim_comparison  = (\n",
    "#     \"\",\n",
    "#     \"non_overlap_k_100\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_opt_wlimit_300\", \"promis_opt_wlimit_1800\"],\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, with_spatial_flip, apply_fit_flips, only_methods, exact_wlim_comparison  = (\n",
    "#     \"\",\n",
    "#     \"5_x_5\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_opt_wlimit_300\", \"promis_opt_wlimit_1800\"],\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, dataset_name, with_spatial_flip, apply_fit_flips, only_methods, exact_wlim_comparison  = (\n",
    "#     \"\",\n",
    "#     \"overlap_k_100_radii_30\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     lar_exp_dir,\n",
    "#     lar_dataset_name,\n",
    "#     True,\n",
    "#     True,\n",
    "#     [\"promis_opt_wlimit_300\", \"promis_opt_wlimit_21600\"],\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi Synthetic Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_non_overlap_k_8\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_5_x_5\",\n",
    "#     \"5_x_5\",\n",
    "#     False,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, apply_fit_flips = (\n",
    "#     \"semi_synthetic_regions_overlap_k_10_radii_4\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"statistical_parity\",\n",
    "#     semi_synth_dir,\n",
    "#     True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Experiment (Equal Opportunity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, only_methods = (\n",
    "#     \"xgb\",\n",
    "#     \"non_overlap_k_8\",\n",
    "#     False,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, only_methods = (\n",
    "#     \"xgb\",\n",
    "#     \"5_x_5\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audit regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_name, partioning_type_name, overlap, fairness_notion, dir_name, only_methods = (\n",
    "#     \"xgb\",\n",
    "#     \"overlap_k_10_radii_4\",\n",
    "#     True,\n",
    "#     \"equal_opportunity\",\n",
    "#     xgb_eq_opp_dir,\n",
    "#     [\"promis_app\", \"promis_opt_wlimit_300\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_path = os.path.join(results_base_path, dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Display Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_to_display_name = {\n",
    "    \"iter\": \"SpatialFlip\",\n",
    "    \"promis_app\": \"PROMIS-Approx\",\n",
    "    \"promis_opt\": \"PROMIS-Direct\",\n",
    "    \"promis_opt_wlimit_300\": \"PROMIS-Direct\",\n",
    "    \"promis_opt_wlimit_1800\": \"PROMIS-Direct (wlimit=1800)\",\n",
    "    \"promis_opt_wlimit_21600\": \"PROMIS-Direct (wlimit=21600)\",\n",
    "    \"init\": \"Base Model\",\n",
    "    \"where\": \"FairWhere\",\n",
    "}\n",
    "\n",
    "if exact_wlim_comparison:\n",
    "    method_to_display_name[\"promis_opt_wlimit_300\"] = \"PROMIS-Direct (wlimit=300)\"\n",
    "\n",
    "method_to_plot_info = {\n",
    "    \"promis_app\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"darkgreen\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"o\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"promis_opt_wlimit_300\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"X\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"promis_opt_wlimit_1800\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"purple\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"X\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"promis_opt_wlimit_21600\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"orange\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"X\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"iter\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"saddlebrown\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \">\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"where\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"red\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"^\",\n",
    "        \"marker_size\": 150,\n",
    "    },\n",
    "    \"init\": {\n",
    "        \"linewidth\": 5,\n",
    "        \"color\": \"blue\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"scatter_marker\": \"*\",\n",
    "        \"marker_size\": 150,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Trained Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    dataset_name, partioning_type_name, clf_name\n",
    ")\n",
    "train_path_info, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, dataset_name\n",
    ")\n",
    "\n",
    "(\n",
    "    val_regions_df,\n",
    "    val_pred_df,\n",
    "    val_labels_df,\n",
    "    y_pred_val,\n",
    "    y_pred_probs_val,\n",
    "    y_true_val,\n",
    "    val_points_per_region,\n",
    "    pos_y_true_indices_val,\n",
    "    pos_points_per_region_val,\n",
    ") = (\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "if dataset_name == \"lar\":\n",
    "    test_regions_df = read_scanned_regs(train_path_info[\"regions\"])\n",
    "    test_pred_df = pd.read_csv(f\"{base_path}preprocess/lar.csv\")\n",
    "    y_pred_test = get_y(test_pred_df, \"label\")\n",
    "    y_true_test = None\n",
    "    y_pred_probs_test=None\n",
    "\n",
    "    test_points_per_region = test_regions_df[\"points\"].tolist()\n",
    "    pos_y_true_indices_test, pos_points_per_region_test = None, None\n",
    "else:\n",
    "    test_regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "    test_pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "    test_labels_df = pd.read_csv(test_path_info[\"labels\"])\n",
    "    y_pred_test = get_y(test_pred_df, \"pred\")\n",
    "    y_pred_probs_test = get_y(test_pred_df, \"prob\") if not clf_name.startswith(\"semi_synthetic\") else None\n",
    "    y_true_test = get_y(test_labels_df, \"label\")\n",
    "\n",
    "    test_points_per_region = test_regions_df[\"points\"].tolist()\n",
    "    pos_y_true_indices_test, pos_points_per_region_test = get_pos_info_regions(\n",
    "        y_true_test, test_points_per_region\n",
    "    )\n",
    "    \n",
    "results_path = f\"{results_base_path}{res_desc_label}/\"\n",
    "\n",
    "save_plots_path = \"\"\n",
    "if save_plots_base_path:\n",
    "    save_plots_path = os.path.join(save_plots_base_path, dir_name, res_desc_label, f\"{fairness_notion}/\")\n",
    "    os.makedirs(save_plots_path, exist_ok=True)\n",
    "    if exact_wlim_comparison:\n",
    "        save_plots_path = os.path.join(save_plots_path, \"exact_wlim_comparison/\")\n",
    "        os.makedirs(save_plots_path, exist_ok=True)\n",
    "    print(f\"Save plots path: {save_plots_path}\")\n",
    "\n",
    "sp_flip_meths_2_pretrained_models = {}\n",
    "if with_spatial_flip:\n",
    "    sp_flip_meths_2_pretrained_models = read_all_models(\n",
    "        f\"{results_path}spatial_flip_models/{fairness_notion}/\",\n",
    "        False,\n",
    "        methods=only_methods,\n",
    "    )\n",
    "\n",
    "sp_opt_meths_2_pretrained_models = read_all_models(\n",
    "    f\"{results_path}spatial_optim_models/{fairness_notion}/\", True, methods=only_methods\n",
    ")\n",
    "all_meths_2_pretrained_models = {\n",
    "    **sp_flip_meths_2_pretrained_models,\n",
    "    **sp_opt_meths_2_pretrained_models,\n",
    "}\n",
    "\n",
    "splitted_labels = get_all_methods_modes_labels(\n",
    "    list(all_meths_2_pretrained_models.keys())\n",
    ")\n",
    "\n",
    "opt_methods_display_labels = splitted_labels[\"opt_labels\"]\n",
    "for label in all_meths_2_pretrained_models.keys():\n",
    "    if label not in method_to_display_name:\n",
    "        method_to_display_name[label] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Predictions - Compute Results Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_methods_to_results_info, budget_range = compute_all_results_info(\n",
    "    all_meths_2_pretrained_models=all_meths_2_pretrained_models,\n",
    "    test_points_per_region=test_points_per_region,\n",
    "    y_pred_test_probs=y_pred_probs_test,\n",
    "    y_true_test=y_true_test,\n",
    "    y_pred_test_orig=y_pred_test,\n",
    "    apply_fit_flips=apply_fit_flips,\n",
    ")\n",
    "\n",
    "sp_flip_methods_2_results_info = {\n",
    "    k: v\n",
    "    for k, v in all_methods_to_results_info.items()\n",
    "    if k in splitted_labels[\"heu_labels\"]\n",
    "}\n",
    "sp_opt_methods_2_results_info = {\n",
    "    k: v\n",
    "    for k, v in all_methods_to_results_info.items()\n",
    "    if k in splitted_labels[\"opt_labels\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test = np.sum(y_pred_test)\n",
    "N_test = len(y_pred_test)\n",
    "RHO_test = P_test / N_test\n",
    "\n",
    "def to_region_dict(pts_per_region):\n",
    "    return [{\"points\": reg} for reg in pts_per_region]\n",
    "\n",
    "init_test_sbi_st_par, init_test_stats_st_par = get_sbi(\n",
    "    y_pred_test, test_points_per_region, with_stats=True\n",
    ")\n",
    "\n",
    "init_test_sbi_eq_opp, init_test_stats_eq_opp = get_sbi(\n",
    "    y_pred_test[pos_y_true_indices_test], pos_points_per_region_test, with_stats=True\n",
    ") if dataset_name != \"lar\" else (None, None)\n",
    "\n",
    "init_acc_test = metrics.accuracy_score(y_true_test, y_pred_test) if y_true_test is not None else None\n",
    "init_f1_test = metrics.f1_score(y_true_test, y_pred_test) if y_true_test is not None else None\n",
    "\n",
    "print(f\"N_test: {N_test}\")\n",
    "print(f\"P_test: {P_test}\")\n",
    "print(f\"RHO_test: {RHO_test:.3f}\")\n",
    "\n",
    "if dataset_name != \"lar\":\n",
    "    TP_test = np.sum(y_pred_test[pos_y_true_indices_test])\n",
    "    TPR_test = TP_test / len(pos_y_true_indices_test)\n",
    "    print(f\"TPR_test: {TPR_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Disparity and Metrics on FairWhere Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr(y_pred):\n",
    "    if len(y_pred) == 0:\n",
    "        return 0\n",
    "    return np.sum(y_pred) / len(y_pred)\n",
    "\n",
    "\n",
    "if clf_name == \"dnn\":\n",
    "    with open(f\"{results_path}{dataset_name}_{fairness_notion}_where_fit_time.txt\", \"r\") as file:\n",
    "        where_fit_time = float(file.read())\n",
    "\n",
    "    where_pred_test_df = pd.read_csv(\n",
    "        f\"{results_path}{dataset_name}_{fairness_notion}_where_model_test_pred.csv\"\n",
    "    )\n",
    "    test_partitioning_id_df = pd.read_csv(\n",
    "        f\"{base_path}partitionings/test_{partioning_name}_partitioning_ids.csv\"\n",
    "    )\n",
    "\n",
    "    y_pred_where_test = get_y(where_pred_test_df, \"pred\")\n",
    "\n",
    "    test_partitioning_id_df[\"id\"] = test_partitioning_id_df[\"id\"].apply(\n",
    "        ast.literal_eval\n",
    "    )\n",
    "    test_partitioning_id_df[\"partitioning\"] = test_partitioning_id_df[\n",
    "        \"partitioning\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    test_ids = test_partitioning_id_df[\"id\"].tolist()\n",
    "    test_partitionings = test_partitioning_id_df[\"partitioning\"].tolist()\n",
    "\n",
    "    (all_methods_to_results_info,\n",
    "    P_where_test,\n",
    "    RHO_where_test,\n",
    "    TP_where_test,\n",
    "    TPR_where_test,\n",
    "    sbi_where_st_par_test,\n",
    "    sbi_where_eq_opp_test,\n",
    "    acc_where_test,\n",
    "    f1_where_test,\n",
    "    init_fairness_loss_list_test,\n",
    "    init_fairness_loss_list_weighted_test,\n",
    "    init_fairness_loss_sum_test,\n",
    "    init_fairness_loss_sum_weighted_test,\n",
    "    where_fairness_loss_list_test,\n",
    "    where_fairness_loss_list_weighted_test,\n",
    "    where_fairness_loss_sum_test,\n",
    "    where_fairness_loss_weighted_sum_test) = compute_avg_disparity_where_metrics(\n",
    "        all_methods_to_results_info,\n",
    "        y_pred_test,\n",
    "        y_pred_where_test,\n",
    "        y_true_test,\n",
    "        test_points_per_region,\n",
    "        pos_points_per_region_test,\n",
    "        test_ids,\n",
    "        test_partitionings,\n",
    "        fair_score_func=metrics.recall_score if fairness_notion == \"equal_opportunity\" else get_pr,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes Maximum Budget Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = compute_max_budget_info(\n",
    "    all_methods_to_results_info=all_methods_to_results_info,\n",
    "    budget_range=budget_range,\n",
    "    dataset_name=dataset_name,\n",
    "    clf_name=clf_name,\n",
    "    partioning_type_name=partioning_type_name,\n",
    "    fairness_notion=fairness_notion,\n",
    "    points_per_region=test_points_per_region,\n",
    "    init_sbi_st_par=init_test_sbi_st_par,\n",
    "    init_sbi_eq_opp=init_test_sbi_eq_opp,\n",
    "    init_stats_st_par=init_test_stats_st_par,\n",
    "    init_stats_eq_opp=init_test_stats_eq_opp,\n",
    "    where_fit_time=where_fit_time if clf_name == \"dnn\" else None,\n",
    "    y_pred_where=y_pred_where_test if clf_name == \"dnn\" else None,\n",
    "    y_true=y_true_test,\n",
    "    sbi_where_st_par=sbi_where_st_par_test if clf_name == \"dnn\" else None,\n",
    "    sbi_where_eq_opp=sbi_where_eq_opp_test if clf_name == \"dnn\" else None,\n",
    "    init_f1=init_f1_test,\n",
    "    f1_where_test=f1_where_test if clf_name == \"dnn\" else None, \n",
    "    init_acc=init_acc_test,\n",
    "    init_fairness_loss_sum=init_fairness_loss_sum_test if clf_name == \"dnn\" else None,\n",
    "    where_fairness_loss_sum=where_fairness_loss_sum_test if clf_name == \"dnn\" else None,\n",
    ")\n",
    "\n",
    "if final_budget_metrics_fname:\n",
    "    file_exists = os.path.isfile(final_budget_metrics_fname)\n",
    "    final_results_df.to_csv(final_budget_metrics_fname, mode='a', index=False, header=not file_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted and Unweighted Disparity Across Partitionings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_name == \"dnn\":\n",
    "    plot_fairness_loss_per_partitioning(\n",
    "        test_ids,\n",
    "        init_fairness_loss_list_test,\n",
    "        where_fairness_loss_list_test,\n",
    "        init_fairness_loss_list_weighted_test,\n",
    "        where_fairness_loss_list_weighted_test,\n",
    "        save_plots_path,\n",
    "        display_title,\n",
    "        \"Test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budgets where PROMIS Methods Reached Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    if \"status\" in all_methods_to_results_info[method].columns:\n",
    "        unique_status = list(all_methods_to_results_info[method][\"status\"].unique())\n",
    "        for status in unique_status:\n",
    "            if status not in [1, 3]:\n",
    "                other_status_exp_idxs = all_methods_to_results_info[method][\n",
    "                    all_methods_to_results_info[method][\"status\"] == status\n",
    "                ][\"exp_idx\"].unique()\n",
    "                print(\n",
    "                    f\"Found status {status} for method {method} for exp indexes: {other_status_exp_idxs}\"\n",
    "                )\n",
    "method_tlimit_cnt = {\n",
    "    method: 0\n",
    "    for method in splitted_labels[\"opt_labels\"]\n",
    "    if \"status\" in all_methods_to_results_info[method].columns\n",
    "}\n",
    "\n",
    "labels = []\n",
    "status_lists = []\n",
    "budget_lists = []\n",
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    if \"status\" in all_methods_to_results_info[method].columns:\n",
    "        labels.append(method)\n",
    "        status_list = all_methods_to_results_info[method][\"status\"].to_list()\n",
    "        budget_list = all_methods_to_results_info[method][\"budget\"].to_list()\n",
    "        tlimit_cnt = len(np.where(np.array(status_list) == 3)[0])\n",
    "        method_tlimit_cnt[method] += tlimit_cnt\n",
    "\n",
    "        status_lists.append(status_list)\n",
    "        budget_lists.append(budget_list)\n",
    "\n",
    "meths_min_C_reach_limit = {}\n",
    "for method in splitted_labels[\"opt_labels\"]:\n",
    "    res_df = all_methods_to_results_info[method]\n",
    "    if 3 in res_df.status.tolist():\n",
    "        meths_min_C_reach_limit[method] = res_df[res_df[\"status\"] == 3][\"budget\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if meths_min_C_reach_limit:\n",
    "    plot_opt_methods_status(\n",
    "        labels=labels,\n",
    "        budget_lists=budget_lists,\n",
    "        status_lists=status_lists,\n",
    "        save_path=save_plots_path,\n",
    "        figsize=figsize,\n",
    "    )\n",
    "\n",
    "    plot_min_C_reach_limit(\n",
    "        meths_min_C_reach_limit,\n",
    "        method_to_plot_info,\n",
    "        method_to_display_name,\n",
    "        opt_methods_display_labels,\n",
    "        figsize=figsize,\n",
    "        save_path=save_plots_path,\n",
    "        display_title=display_title,\n",
    "\n",
    "    )\n",
    "else:\n",
    "    print(\"No PROMIS method reached the work limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBI per budget for PROMIS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_sbis = {\n",
    "    \"Budget\": budget_range\n",
    "}\n",
    "\n",
    "for method in all_methods_to_results_info.keys():\n",
    "    method_sbis = []\n",
    "    sbi_label = \"sbi_eq_opp_test\" if fairness_notion == \"equal_opportunity\" else \"sbi_st_par_test\"\n",
    "    for budget in budget_range:\n",
    "        res_df = all_methods_to_results_info[method]\n",
    "        sbi = res_df[res_df[\"budget\"] == budget][sbi_label].values[0]\n",
    "        method_sbis.append(sbi)\n",
    "    budget_sbis[method_to_display_name[method]] = method_sbis\n",
    "if \"PROMIS-Direct\" in budget_sbis:\n",
    "    budget_sbis[\"PROMIS-Direct (wlimit=300)\"] = budget_sbis[\"PROMIS-Direct\"]\n",
    "    del budget_sbis[\"PROMIS-Direct\"]\n",
    "budget_sbis_df = pd.DataFrame(budget_sbis)\n",
    "sorted_methods = sorted(list(budget_sbis.keys()))\n",
    "sorted_methods.remove(\"Budget\")\n",
    "budget_sbis_df = budget_sbis_df[[\"Budget\"] + sorted_methods]\n",
    "display(budget_sbis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_method_to_plot_info = method_to_plot_info.copy()\n",
    "for method in method_to_plot_info.keys():\n",
    "    cur_method_to_plot_info[method][\"marker_size\"] = 80\n",
    "    cur_method_to_plot_info[method][\"linewidth\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flips_limit = None\n",
    "if fairness_notion == \"statistical_parity\":\n",
    "    plot_scores(\n",
    "        methods_to_res_info=all_methods_to_results_info,\n",
    "        init_sbi=init_test_sbi_st_par,\n",
    "        method_to_plot_info=method_to_plot_info if display_title else cur_method_to_plot_info,\n",
    "        method_to_display_name=method_to_display_name,\n",
    "        save_plots_path=save_plots_path,\n",
    "        figsize=figsize if display_title else (4.7, 2.8),\n",
    "        flips_limit=flips_limit,\n",
    "        append_to_title=\" (Statistical Parity - Test Set)\",\n",
    "        append_to_save=\"_st_par_test\",\n",
    "        score_label=\"sbi_st_par_test\",\n",
    "        display_title=display_title,\n",
    "        other_sbi=sbi_where_st_par_test if clf_name == \"dnn\" else None,\n",
    "        other_sbi_method=\"where\" if clf_name == \"dnn\" else None,\n",
    "    )\n",
    "\n",
    "if fairness_notion == \"equal_opportunity\":\n",
    "    plot_scores(\n",
    "        methods_to_res_info=all_methods_to_results_info,\n",
    "        init_sbi=init_test_sbi_eq_opp,\n",
    "        method_to_plot_info=method_to_plot_info if display_title else cur_method_to_plot_info,\n",
    "        method_to_display_name=method_to_display_name,\n",
    "        save_plots_path=save_plots_path,\n",
    "        figsize=figsize if display_title else (4.7, 2.8),\n",
    "        flips_limit=flips_limit,\n",
    "        append_to_title=\" (Equal Opportunity - Test Set)\",\n",
    "        append_to_save=\"_eq_opp_test\",\n",
    "        score_label=\"sbi_eq_opp_test\",\n",
    "        display_title=display_title,\n",
    "        other_sbi=sbi_where_eq_opp_test if clf_name == \"dnn\" else None,\n",
    "        other_sbi_method=\"where\" if clf_name == \"dnn\" else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness and Performance Metrics per Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scores = {\n",
    "    \"sbi_st_par\": {\n",
    "        \"test\": init_test_sbi_st_par,\n",
    "    },\n",
    "    \"sbi_eq_opp\": {\n",
    "        \"test\": init_test_sbi_eq_opp,\n",
    "    },\n",
    "    \"accuracy\": {\n",
    "        \"test\": init_acc_test,\n",
    "    },\n",
    "    \"f1\": {\n",
    "        \"test\": init_f1_test,\n",
    "    },\n",
    "    \"fair_loss_sum\":\n",
    "    {\n",
    "        \"test\": init_fairness_loss_sum_test if clf_name == \"dnn\" else None,\n",
    "    }\n",
    "}\n",
    "\n",
    "fair_scores_display_labels = {\n",
    "    \"sbi_st_par\": \"SBI\",\n",
    "    \"sbi_eq_opp\": \"SBI\",\n",
    "    \"fair_loss_sum\": \"MeanDev\"\n",
    "}\n",
    "performance_scores_display_labels = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"f1\": \"F1 Score\",\n",
    "}\n",
    "sets_display_labels = {\n",
    "    \"sol\": \"Solution\",\n",
    "    \"val\": \"Validation Set\",\n",
    "    \"test\": \"Test Set\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_title:\n",
    "    score1_vs_score2_figsize = (9, 14)  \n",
    "elif clf_name == \"dnn\":\n",
    "    score1_vs_score2_figsize = (4, 7)\n",
    "else:\n",
    "    score1_vs_score2_figsize = (4, 4.5)\n",
    "\n",
    "if y_true_test is not None:\n",
    "    sets = [\"test\"]\n",
    "    if fairness_notion == \"statistical_parity\":\n",
    "        fair_scores = [\"sbi_st_par\"]\n",
    "    else:\n",
    "        fair_scores = [\"sbi_eq_opp\"]\n",
    "\n",
    "    if clf_name == \"dnn\":\n",
    "        where_scores = {\n",
    "            \"sbi_st_par\": {\n",
    "                \"test\": sbi_where_st_par_test,\n",
    "            },\n",
    "            \"sbi_eq_opp\": {\n",
    "                \"test\": sbi_where_eq_opp_test,\n",
    "            },\n",
    "            \"accuracy\": {\n",
    "                \"test\": acc_where_test,\n",
    "            },\n",
    "            \"f1\": {\n",
    "                \"test\": f1_where_test,\n",
    "            },\n",
    "            \"fair_loss_sum\":\n",
    "            {\n",
    "                \"test\": where_fairness_loss_sum_test,\n",
    "            }\n",
    "        }\n",
    "        fair_scores.append(\"fair_loss_sum\")\n",
    "        performance_scores = [\"f1\"]\n",
    "    else:\n",
    "        performance_scores = [\"accuracy\"]\n",
    "\n",
    "    for set_ in sets:\n",
    "        for performance_score in performance_scores:\n",
    "\n",
    "            # set the min and max axis values for the performance score\n",
    "            # as the global min and max values per group of experiments \n",
    "            # (e.g., DNN considering statistical parity)\n",
    "\n",
    "            if clf_name == \"dnn\":\n",
    "                if fairness_notion == \"statistical_parity\":\n",
    "                    score_2_min_axis = 0.3868\n",
    "                    score_2_max_axis = 0.4903\n",
    "                else:\n",
    "                    score_2_min_axis = 0.4332\n",
    "                    score_2_max_axis = 0.4880\n",
    "            elif clf_name == \"xgb\":\n",
    "                score_2_min_axis = 0.7276\n",
    "                score_2_max_axis = 0.7288\n",
    "            else:\n",
    "                score_2_min_axis = None\n",
    "                score_2_max_axis = None\n",
    "\n",
    "            if score_2_max_axis is not None and score_2_min_axis is not None:\n",
    "                score_2_min_axis = score_2_min_axis - 0.006\n",
    "                score_2_max_axis = score_2_max_axis + 0.006\n",
    "\n",
    "            plot_score1_vs_score2(\n",
    "                methods_to_res_info=all_methods_to_results_info,\n",
    "                score_label1=f\"{fair_scores[0]}_{set_}\",\n",
    "                score_label2=f\"{performance_score}_{set_}\",\n",
    "                score_label3=f\"{fair_scores[1]}_{set_}\" if len(fair_scores) > 1 else None,\n",
    "                score_display_label1=fair_scores_display_labels[fair_scores[0]],\n",
    "                score_display_label2=performance_scores_display_labels[performance_score],\n",
    "                score_display_label3=fair_scores_display_labels[fair_scores[1]] if len(fair_scores) > 1 else None,\n",
    "                init_score1=init_scores[fair_scores[0]][set_],\n",
    "                init_score2=init_scores[performance_score][set_],\n",
    "                init_score3=init_scores[fair_scores[1]][set_] if len(fair_scores) > 1 else None,\n",
    "                method_to_plot_info=method_to_plot_info,\n",
    "                method_to_display_name=method_to_display_name,\n",
    "                save_plots_path=save_plots_path,\n",
    "                figsize=score1_vs_score2_figsize,\n",
    "                append_to_title=f\" ({sets_display_labels[set_]})\",\n",
    "                display_title=display_title,\n",
    "                other_score1=where_scores[fair_scores[0]][set_] if clf_name == \"dnn\" else None,\n",
    "                other_score2=where_scores[performance_score][set_] if clf_name == \"dnn\" else None,\n",
    "                other_score_3=where_scores[fair_scores[1]][set_] if clf_name == \"dnn\" and len(fair_scores) > 1 else None,\n",
    "                other_method=\"where\" if clf_name == \"dnn\" else None,\n",
    "                score_2_min_axis=score_2_min_axis,\n",
    "                score_2_max_axis=score_2_max_axis,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive, Positive Rate, Actual Flips per Method per Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_flips_where_test = (\n",
    "    np.sum(np.abs(y_pred_where_test - y_pred_test)) if clf_name == \"dnn\" else None\n",
    ")\n",
    "actual_pos_flips_where_test = (\n",
    "    np.sum(\n",
    "        np.abs(\n",
    "            y_pred_where_test[pos_y_true_indices_test]\n",
    "            - y_pred_test[pos_y_true_indices_test]\n",
    "        )\n",
    "    )\n",
    "    if clf_name == \"dnn\"\n",
    "    else None\n",
    ")\n",
    "plot_compare_methods_info(\n",
    "    all_methods_to_results_info,\n",
    "    P_test,\n",
    "    RHO_test,\n",
    "    p_label=\"P_test\",\n",
    "    rho_label=\"RHO_test\",\n",
    "    actual_flips_label=\"actual_flips_test\",\n",
    "    method_to_plot_info=method_to_plot_info if display_title else cur_method_to_plot_info,\n",
    "    method_to_display_name=method_to_display_name,\n",
    "    save_path=save_plots_path,\n",
    "    figsize=figsize,\n",
    "    append_to_title=f\" ({fairness_notion} - Test Set)\",\n",
    "    display_title=display_title,\n",
    "    other_P=P_where_test if clf_name == \"dnn\" else None,\n",
    "    other_RHO=RHO_where_test if clf_name == \"dnn\" else None,\n",
    "    other_actual_flips=actual_flips_where_test,\n",
    "    other_actual_pos_flips=actual_pos_flips_where_test,\n",
    "    other_method=\"where\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_labels = final_results_df[\"Method\"].unique().tolist()\n",
    "stats_per_method = (\n",
    "    final_results_df.groupby(\"Method\", sort=False)[\"Statistics\"]\n",
    "    .apply(list)\n",
    "    .tolist()\n",
    ")\n",
    "stats_per_method = [np.concatenate(stats).tolist() for stats in stats_per_method]\n",
    "\n",
    "if not display_title:\n",
    "    if partioning_name.startswith(\"regions_5_x_5\"):\n",
    "        figsize = (14, 2.5)\n",
    "    elif partioning_name.startswith(\"regions_overlap\"):\n",
    "        figsize = (10, 2.5)\n",
    "    else:\n",
    "        figsize = (4.5, 2.5)\n",
    "\n",
    "plot_regions_norm_stats(\n",
    "    methods_stats=stats_per_method,\n",
    "    methods_labels=methods_labels,\n",
    "    xlabel=\"Regions\",\n",
    "    save_path=save_plots_path,\n",
    "    append_to_title=\"(Test Set)\",\n",
    "    display_title=display_title,\n",
    "    method_to_display_name=method_to_display_name,\n",
    "    method_to_plot_info=method_to_plot_info,\n",
    "    figsize=figsize,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMIS Env",
   "language": "python",
   "name": "promis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
