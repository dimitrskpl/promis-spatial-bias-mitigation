{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to generate the times and additional metrics (f1, accuracy, mean disparity) for the paper for the final budget. You may run this notebook after running all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"final_budget_metrics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process the final budget metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_budget_metrics_df = pd.read_csv(fname)\n",
    "\n",
    "audit_regions_name_dict = {\n",
    "    \"Non-Overlapping KMeans\": \"Clusters\",\n",
    "    \"Overlapping KMeans\": \"Scan Regions\",\n",
    "    \"Overlapping Partitionings\": \"Grids\",\n",
    "}\n",
    "fairness_notion_name_dict = {\n",
    "    \"Equal Opportunity\": \"EO\",\n",
    "    \"Statistical Parity\": \"SP\",\n",
    "}\n",
    "final_budget_metrics_df[\"Audit Regions\"] = final_budget_metrics_df[\"Audit Regions\"].map(\n",
    "    audit_regions_name_dict\n",
    ")\n",
    "final_budget_metrics_df[\"Fairness Notion\"] = final_budget_metrics_df[\n",
    "    \"Fairness Notion\"\n",
    "].map(fairness_notion_name_dict)\n",
    "\n",
    "# combine experiment columns info to create unique ids for each experiment\n",
    "final_budget_metrics_df[\"exp_desc\"] = (\n",
    "    final_budget_metrics_df[\"Dataset\"]\n",
    "    + \"_\"\n",
    "    + final_budget_metrics_df[\"Classifier\"]\n",
    "    + \"_\"\n",
    "    + final_budget_metrics_df[\"Audit Regions\"]\n",
    "    + \"_\"\n",
    "    + final_budget_metrics_df[\"Fairness Notion\"]\n",
    ")\n",
    "# remove the results of the Promis-Exact method with high work limit\n",
    "final_budget_metrics_df = final_budget_metrics_df[\n",
    "    final_budget_metrics_df[\"Method\"] != \"promis_opt_wlimit_1800\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"init\",\n",
    "    \"iter\",\n",
    "    \"promis_opt_wlimit_300\",\n",
    "    \"promis_app\",\n",
    "    \"FairWhere\",\n",
    "]\n",
    "\n",
    "final_res = {\n",
    "    \"Dataset\": [],\n",
    "    \"Classifier\": [],\n",
    "    \"Audit Regions\": [],\n",
    "    \"Fairness\": [],\n",
    "    \"Budget\": [],\n",
    "    \"Init MLR\": [],\n",
    "    \"PROMIS-Direct MLR\": [],\n",
    "    \"PROMIS-Approx MLR\": [],\n",
    "    \"FairWhere MLR\": [],\n",
    "    \"SpatialFlip MLR\": [],\n",
    "    \"Init Mean Disparity\": [],\n",
    "    \"PROMIS-Direct Mean Disparity\": [],\n",
    "    \"PROMIS-Approx Mean Disparity\": [],\n",
    "    \"FairWhere Mean Disparity\": [],\n",
    "    \"SpatialFlip Mean Disparity\": [],\n",
    "    \"PROMIS-Direct Time\": [],\n",
    "    \"PROMIS-Approx Time\": [],\n",
    "    \"FairWhere Time\": [],\n",
    "    \"SpatialFlip Time\": [],\n",
    "    \"Init Accuracy\": [],\n",
    "    \"PROMIS-Direct Accuracy\": [],\n",
    "    \"PROMIS-Approx Accuracy\": [],\n",
    "    \"FairWhere Accuracy\": [],\n",
    "    \"SpatialFlip Accuracy\": [],\n",
    "    \"Init F1\": [],\n",
    "    \"PROMIS-Direct F1\": [],\n",
    "    \"PROMIS-Approx F1\": [],\n",
    "    \"FairWhere F1\": [],\n",
    "    \"SpatialFlip F1\": [],\n",
    "}\n",
    "methods_to_labels = {\n",
    "    'init': \"Init\", \n",
    "    'iter': \"SpatialFlip\", \n",
    "    'promis_opt_wlimit_300': \"PROMIS-Direct\", \n",
    "    'promis_app': \"PROMIS-Approx\", \n",
    "    'FairWhere': \"FairWhere\"\n",
    "}\n",
    "\n",
    "# fill in the final results dataframe\n",
    "for exp_desc in final_budget_metrics_df['exp_desc'].unique():\n",
    "    exp_df = final_budget_metrics_df[final_budget_metrics_df['exp_desc'] == exp_desc]\n",
    "    final_res[\"Dataset\"].append(exp_df['Dataset'].values[-1])\n",
    "    final_res[\"Classifier\"].append(exp_df['Classifier'].values[-1])\n",
    "    final_res[\"Audit Regions\"].append(exp_df['Audit Regions'].values[-1])\n",
    "    final_res[\"Fairness\"].append(exp_df['Fairness Notion'].values[-1])\n",
    "    final_res[\"Budget\"].append(exp_df['Budget'].values[-1])\n",
    "    for method in methods:\n",
    "        method_df = exp_df[exp_df['Method'] == method]\n",
    "        \n",
    "        for metric in [\"Time\", \"MLR\", \"Mean Disparity\", \"Accuracy\", \"F1\"]:\n",
    "            label = methods_to_labels[method] + \" \" + metric\n",
    "            if method_df.empty:\n",
    "                final_res[label].append(None)\n",
    "            else:\n",
    "                if metric == \"Time\":\n",
    "                    if method != \"init\":\n",
    "                        final_res[label].append(method_df[\"Time\"].values[0])\n",
    "                else:\n",
    "                    final_res[label].append(method_df[metric].values[0])\n",
    "            \n",
    "final_res_df = pd.DataFrame(final_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the results for each set of experiments\n",
    "\n",
    "DNN_mlr_exp_df = final_res_df[final_res_df['Classifier'] == 'DNN'][[\"Audit Regions\", \"Fairness\", \"Budget\", \"Init MLR\", \"PROMIS-Direct MLR\", \"PROMIS-Approx MLR\", \"FairWhere MLR\", \"SpatialFlip MLR\"]]\n",
    "DNN_mean_disparity_exp_df = final_res_df[final_res_df['Classifier'] == 'DNN'][[\"Audit Regions\", \"Fairness\", \"Budget\", \"Init Mean Disparity\", \"PROMIS-Direct Mean Disparity\", \"PROMIS-Approx Mean Disparity\", \"FairWhere Mean Disparity\", \"SpatialFlip Mean Disparity\"]]\n",
    "DNN_f1_exp_df = final_res_df[final_res_df['Classifier'] == 'DNN'][[\"Audit Regions\", \"Fairness\", \"Budget\", \"Init F1\", \"PROMIS-Direct F1\", \"PROMIS-Approx F1\", \"FairWhere F1\", \"SpatialFlip F1\"]]\n",
    "DNN_times_df = final_res_df[final_res_df['Classifier'] == 'DNN'][[\"Audit Regions\", \"Fairness\", \"Budget\", \"PROMIS-Direct Time\", \"PROMIS-Approx Time\", \"FairWhere Time\", \"SpatialFlip Time\"]]\n",
    "\n",
    "LAR_mlr_exp_df = final_res_df[final_res_df['Dataset'] == 'LAR'][[\"Audit Regions\", \"Budget\", \"Init MLR\", \"PROMIS-Direct MLR\", \"PROMIS-Approx MLR\", \"SpatialFlip MLR\"]]\n",
    "LAR_times_df = final_res_df[final_res_df['Dataset'] == 'LAR'][[\"Audit Regions\", \"Budget\", \"PROMIS-Direct Time\", \"PROMIS-Approx Time\", \"SpatialFlip Time\"]]\n",
    "\n",
    "synth_mlr_exp_df = final_res_df[final_res_df['Classifier'] == 'Unfair by Design'][[\"Audit Regions\", \"Budget\", \"Init MLR\", \"PROMIS-Direct MLR\", \"PROMIS-Approx MLR\", \"SpatialFlip MLR\"]]\n",
    "synth_times_df = final_res_df[final_res_df['Classifier'] == 'Unfair by Design'][[\"Audit Regions\", \"Budget\", \"PROMIS-Direct Time\", \"PROMIS-Approx Time\", \"SpatialFlip Time\"]]\n",
    "\n",
    "XGB_mlr_exp_df = final_res_df[final_res_df['Classifier'] == 'XGB'][[\"Audit Regions\", \"Budget\", \"Init MLR\", \"PROMIS-Direct MLR\", \"PROMIS-Approx MLR\"]]\n",
    "XGB_acc_exp_df = final_res_df[final_res_df['Classifier'] == 'XGB'][[\"Audit Regions\", \"Budget\", \"Init Accuracy\", \"PROMIS-Direct Accuracy\", \"PROMIS-Approx Accuracy\"]]\n",
    "XGB_times_df = final_res_df[final_res_df['Classifier'] == 'XGB'][[\"Audit Regions\", \"Budget\", \"PROMIS-Direct Time\", \"PROMIS-Approx Time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Times for All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x):\n",
    "    if isinstance(x, (int, float)):  \n",
    "        return str(int(round(x))) if x >= 1 else str(round(x, 2))\n",
    "    return x  \n",
    "times = final_res_df[[\"Dataset\",\"Audit Regions\", \"Classifier\", \"Fairness\", 'PROMIS-Direct Time', 'PROMIS-Approx Time', 'FairWhere Time',\n",
    "       'SpatialFlip Time'\t]].copy()\n",
    "times.fillna(\"-\", inplace=True)\n",
    "numeric_columns = ['PROMIS-Direct Time', 'PROMIS-Approx Time', 'FairWhere Time', 'SpatialFlip Time']\n",
    "times[numeric_columns] = times[numeric_columns].applymap(custom_round)\n",
    "\n",
    "display(times.drop_index().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Metrics for each set of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DNN MLR\")\n",
    "display(DNN_mlr_exp_df)\n",
    "\n",
    "print(\"DNN Mean Disparity\")\n",
    "display(DNN_mean_disparity_exp_df)\n",
    "\n",
    "print(\"DNN F1\")\n",
    "display(DNN_f1_exp_df)\n",
    "\n",
    "print(\"DNN Times\")\n",
    "display(DNN_times_df)\n",
    "\n",
    "print(\"LAR MLR\")\n",
    "display(LAR_mlr_exp_df)\n",
    "\n",
    "print(\"LAR Times\")\n",
    "display(LAR_times_df)\n",
    "\n",
    "print(\"Synth MLR\")\n",
    "display(synth_mlr_exp_df)\n",
    "\n",
    "print(\"Synth Times\")\n",
    "display(synth_times_df)\n",
    "\n",
    "print(\"XGB MLR\")\n",
    "display(XGB_mlr_exp_df)\n",
    "\n",
    "print(\"XGB Accuracy\")\n",
    "display(XGB_acc_exp_df)\n",
    "\n",
    "print(\"XGB Times\")\n",
    "display(XGB_times_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMIS Env",
   "language": "python",
   "name": "promis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
