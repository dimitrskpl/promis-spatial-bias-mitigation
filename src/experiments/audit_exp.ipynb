{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs audit for all different settings models/audit regions using the scan statistic approach where monte carlo simulation are performed to define the significant thresholds. We use the original scan statistic method from the audit paper which defines the statistic considering the maximum ligelikoods of the inside=outside hypothesis and the inside!=outside hypothesis. We consider the outputs as ground truth, i.e. which regions are classified as significant/non-significant. So the ground truth is 0/1 for each of the audit regions. Additionally we perform audit where we replace the statistic formula with the PROMIS approximation formula. Finally we compute the of accuracy PROMIS approximation for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils.stats_utils import get_signif_threshold, scan_regions\n",
    "from utils.data_utils import  get_y\n",
    "from utils.results_names_utils import combine_world_info, get_train_val_test_paths\n",
    "from utils.data_utils import read_scanned_regs\n",
    "from tqdm import tqdm\n",
    "from utils.data_utils import get_pos_info_regions\n",
    "from utils.stats_utils import get_random_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/\"\n",
    "xgb_clf_name = \"xgb\"\n",
    "dnn_clf_name = \"dnn\"\n",
    "unfair_clf_name = \"semi_synthetic\"\n",
    "crime_dataset_name = \"crime\"\n",
    "lar_dataset_name = \"lar\"\n",
    "non_over_partioning_type_name = \"non_overlap_k_8\"\n",
    "over_partioning_type_name = \"overlap_k_10_radii_4\"\n",
    "grid_partitioning_type_name = \"5_x_5\"\n",
    "non_over_partioning_type_name_lar = \"non_overlap_k_100\"\n",
    "over_partioning_type_name_lar = \"overlap_k_100_radii_30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "signif_level = 0.005\n",
    "n_alt_worlds = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Â Approximation Audit Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regs_norm_counts(points_per_region):\n",
    "    \"\"\"\n",
    "    Calculate normalized region membership counts for each region.\n",
    "\n",
    "    This function counts how many times each point belongs to a region, \n",
    "    and then computes a normalized \"weight\" for each region based on \n",
    "    the inverse of that membership count.\n",
    "\n",
    "    Args:\n",
    "        points_per_region (list of lists): \n",
    "            A list where each element is a list of point indices \n",
    "            corresponding to a region.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of normalized counts (floats) for each region.\n",
    "              Specifically, for each region, the sum over all its points \n",
    "              of (1 / count of regions that point belongs to).\n",
    "    \"\"\"\n",
    "\n",
    "    point_idx_to_regs_cnt = {}\n",
    "    for pts in points_per_region:\n",
    "        for point in pts:\n",
    "            if point in point_idx_to_regs_cnt:\n",
    "                point_idx_to_regs_cnt[point] += 1\n",
    "            else:\n",
    "                point_idx_to_regs_cnt[point] = 1\n",
    "\n",
    "    norm_regions_cnts = []\n",
    "    for pts in points_per_region:\n",
    "        region_pts_weights_sum = 0\n",
    "        for point in pts:\n",
    "            region_pts_weights_sum += 1 / point_idx_to_regs_cnt[point]\n",
    "        norm_regions_cnts.append(region_pts_weights_sum)\n",
    "\n",
    "    return norm_regions_cnts\n",
    "\n",
    "def compute_promis_app(w, n, p, N, P):\n",
    "    \"\"\"\n",
    "    Compute the inside-outside statistic for a given region.\n",
    "\n",
    "    This statistic compares the proportion of \"positive\" points (p) \n",
    "    inside a region to the proportion of positive points (p_out) \n",
    "    outside the region. \n",
    "\n",
    "    Args:\n",
    "        w (float): A weight factor for the region.\n",
    "        n (int): Number of points inside the region.\n",
    "        p (int): Number of positive points inside the region.\n",
    "        N (int): Total number of points.\n",
    "        P (int): Total number of positive points.\n",
    "\n",
    "    Returns:\n",
    "        float: The absolute difference between inside and outside proportions, \n",
    "               scaled by the weight w. Returns 0 if `n` or `n_out` is 0.\n",
    "    \"\"\"\n",
    "    n_out = N - n\n",
    "    \n",
    "    if n == 0 or n_out == 0:\n",
    "        return 0  \n",
    "    \n",
    "    p_out = P - p\n",
    "    \n",
    "    pr = p / n\n",
    "    pr_out = p_out / n_out\n",
    "    \n",
    "    return np.abs(w * (pr - pr_out))\n",
    "              \n",
    "def get_signif_thresh_scanned_regions(\n",
    "    signif_level, n_alt_worlds, regions, y_pred, N, P, seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Determine the significance threshold for each region using a Monte Carlo approach \n",
    "    and scan all regions to label them as significant or not.\n",
    "\n",
    "    Internally, it calls `get_signif_threshold` to compute the threshold from \n",
    "    alternative worlds, and `scan_regions` to compute the actual statistics.\n",
    "\n",
    "    Args:\n",
    "        signif_level (float): Significance level (e.g., 0.05).\n",
    "        n_alt_worlds (int): Number of alternative (random) worlds to generate for threshold.\n",
    "        regions (list): A list of region dictionaries. Each dictionary has a \"points\" key \n",
    "                        containing indices of points in that region.\n",
    "        y_pred (array-like): Array of binary predictions (0 or 1) for each point.\n",
    "        N (int): Total number of points.\n",
    "        P (int): Total number of positive points in y_pred.\n",
    "        seed (int, optional): Seed for random number generator. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "            - 'signif': boolean indicating whether the region is significant.\n",
    "            - 'statistic': the statistic value for that region.\n",
    "            - 'signif_thresh': the significance threshold used.\n",
    "    \"\"\"\n",
    "    signif_thresh = get_signif_threshold(\n",
    "        signif_level, n_alt_worlds, regions, N, P, seed\n",
    "    )\n",
    "\n",
    "    _, _, statistics = scan_regions(regions, y_pred, N, P, verbose=False)\n",
    "\n",
    "    scanned_regions = []\n",
    "    for i in range(len(regions)):\n",
    "        signif = False\n",
    "        if statistics[i] >= signif_thresh:\n",
    "            signif = True\n",
    "\n",
    "        reg = {\n",
    "            \"signif\": signif,\n",
    "            \"statistic\": statistics[i],\n",
    "            \"signif_thresh\": signif_thresh,\n",
    "        }\n",
    "        scanned_regions.append(reg)\n",
    "\n",
    "    df_scanned_regs = pd.DataFrame(scanned_regions)\n",
    "\n",
    "    return df_scanned_regs\n",
    "def scan_promis_app_regions(regions, weights, types, N, P):\n",
    "    \"\"\"\n",
    "    Compute the inside-outside statistic for all regions and identify \n",
    "    the region with the maximum statistic.\n",
    "\n",
    "    Args:\n",
    "        regions (list of dict): List of region dictionaries, \n",
    "            each with a \"points\" key containing point indices.\n",
    "        weights (list of float): List of precomputed weights for each region.\n",
    "        types (array-like): Binary array (0 or 1) representing the label/type for each point.\n",
    "        N (int): Total number of points.\n",
    "        P (int): Total number of positive points in `types`.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_region, max_likelihood, statistics)\n",
    "            - best_region (dict): The region (dictionary) with the highest statistic.\n",
    "            - max_likelihood (float): The value of the highest statistic.\n",
    "            - statistics (list of float): The statistic values for all regions.\n",
    "    \"\"\"\n",
    "    statistics = []\n",
    "\n",
    "    for i in range(len(regions)):\n",
    "        region = regions[i]\n",
    "        n = len(region[\"points\"])\n",
    "        p = np.sum(types[region[\"points\"]])\n",
    "        statistics.append(compute_promis_app(weights[i], n, p, N, P))\n",
    "\n",
    "    idx = np.argmax(statistics)\n",
    "    max_likelihood = statistics[idx]\n",
    "\n",
    "    return regions[idx], max_likelihood, statistics\n",
    "\n",
    "def scan_promis_app_alt_worlds(n_alt_worlds, regions, weights, N, P, seed=None):\n",
    "    \"\"\"\n",
    "    For each alternative world (a random realization of types), compute \n",
    "    the inside-outside statistic for all regions, identify the region \n",
    "    with the maximum statistic, and keep track of it.\n",
    "\n",
    "    Args:\n",
    "        n_alt_worlds (int): Number of alternative (random) worlds to generate.\n",
    "        regions (list of dict): List of region dictionaries.\n",
    "        weights (list of float): List of weights for each region.\n",
    "        N (int): Total number of points.\n",
    "        P (int): Total number of positive points in the original setting.\n",
    "        seed (int, optional): Random seed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (alt_worlds, best_statistic_overall)\n",
    "            - alt_worlds (list of tuples): Each tuple has the form \n",
    "              (alt_types, alt_best_region, alt_max_likeli).\n",
    "            - best_statistic_overall (float): The maximum statistic observed \n",
    "              among all alternative worlds (the top of alt_worlds when sorted).\n",
    "    \"\"\"\n",
    "    alt_worlds = []\n",
    "    current_seed = seed\n",
    "\n",
    "    for _ in tqdm(range(n_alt_worlds), desc=\"Monte Carlo simulations\"):\n",
    "        alt_types = get_random_types(N, P, current_seed)\n",
    "        cur_P = np.sum(alt_types)\n",
    "        alt_best_region, alt_max_likeli, _ = scan_promis_app_regions(\n",
    "            regions, weights, alt_types, N, cur_P\n",
    "        )\n",
    "        alt_worlds.append((alt_types, alt_best_region, alt_max_likeli))\n",
    "\n",
    "        if current_seed is not None:\n",
    "            current_seed += 1\n",
    "\n",
    "    alt_worlds.sort(key=lambda x: -x[2])\n",
    "\n",
    "    return alt_worlds, alt_worlds[0][2]\n",
    "\n",
    "def get_promis_app_signif_threshold(\n",
    "    signif_level, n_alt_worlds, regions, weights, N, P, seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a significance threshold for the inside-outside statistic via Monte Carlo simulations.\n",
    "\n",
    "    The threshold is determined by generating `n_alt_worlds` alternative worlds, \n",
    "    computing the maximum statistic among the regions in each world, and then \n",
    "    finding the statistic value at the `signif_level` quantile.\n",
    "\n",
    "    Args:\n",
    "        signif_level (float): Significance level (e.g., 0.05).\n",
    "        n_alt_worlds (int): Number of alternative worlds to generate.\n",
    "        regions (list of dict): List of region dictionaries.\n",
    "        weights (list of float): Weights for each region.\n",
    "        N (int): Total number of points.\n",
    "        P (int): Total number of positive points.\n",
    "        seed (int, optional): Random seed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: Significance threshold for the inside-outside statistic.\n",
    "    \"\"\"\n",
    "    alt_worlds, _ = scan_promis_app_alt_worlds(n_alt_worlds, regions, weights, N, P, seed)\n",
    "\n",
    "    k = int(signif_level * n_alt_worlds)\n",
    "\n",
    "    signif_thresh = alt_worlds[k][2]  \n",
    "\n",
    "    return signif_thresh\n",
    "\n",
    "def spatial_promis_app_scan_statistic(points_per_region, y_pred, weights,label, n_alt_worlds=1000, signif_level=0.001, seed=None):\n",
    "    \"\"\"\n",
    "    Performs the inside-outside scan statistic on a set of regions and determines \n",
    "    which regions are significant based on a Monte Carlo-derived threshold.\n",
    "\n",
    "    Args:\n",
    "        points_per_region (list of lists): A list where each element is a list of \n",
    "            point indices corresponding to a region.\n",
    "        y_pred (array-like): Binary predictions (0 or 1) for each point.\n",
    "        weights (list of float): List of weights corresponding to each region.\n",
    "        label (str): Label name to differentiate multiple scan statistic runs.\n",
    "        n_alt_worlds (int, optional): Number of alternative worlds to generate. Defaults to 1000.\n",
    "        signif_level (float, optional): Significance level. Defaults to 0.05.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns:\n",
    "            - '{label}_statistic': Observed statistic for each region.\n",
    "            - '{label}_signif_thresh': Significance threshold used.\n",
    "            - '{label}_signif': Boolean indicating significant regions.\n",
    "    \"\"\"\n",
    "    N = len(y_pred)\n",
    "    P = np.sum(y_pred)\n",
    "    y_pred = y_pred.copy()\n",
    "    regions = [{\"points\": region} for region in points_per_region]\n",
    "\n",
    "    scanned_regions = []\n",
    "    \n",
    "    observed_stats = []\n",
    "    for i in range(len(points_per_region)):\n",
    "        pts = points_per_region[i]\n",
    "        n = len(pts)\n",
    "        p = np.sum(y_pred[pts])\n",
    "        I_Z = compute_promis_app(weights[i], n, p, N, P)\n",
    "        observed_stats.append(I_Z)\n",
    "    \n",
    "    signif_thresh = get_promis_app_signif_threshold(\n",
    "        signif_level, n_alt_worlds, regions, weights, N, P, seed\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for observed_I in observed_stats:\n",
    "        signif = False\n",
    "        if observed_I >= signif_thresh:\n",
    "            signif = True\n",
    "\n",
    "        scanned_regions.append({\n",
    "            f\"{label}_statistic\": observed_I,\n",
    "            f\"{label}_signif_thresh\": signif_thresh,\n",
    "            f\"{label}_signif\": signif\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(scanned_regions)\n",
    "\n",
    "def get_scores(true_signif, pred_signif):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix scores (TP, FP, TN, FN) for two binary arrays.\n",
    "\n",
    "    Args:\n",
    "        true_signif (array-like): Ground truth significance array (boolean).\n",
    "        pred_signif (array-like): Predicted significance array (boolean).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tp, fp, tn, fn), where each is an integer.\n",
    "    \"\"\"\n",
    "    tp = np.sum(np.logical_and(true_signif, pred_signif))\n",
    "    fp = np.sum(np.logical_and(np.logical_not(true_signif), pred_signif))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(true_signif), np.logical_not(pred_signif)))\n",
    "    fn = np.sum(np.logical_and(true_signif, np.logical_not(pred_signif)))\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def run_scan_methods(y_pred, points_per_region, signif_level=0.005, n_alt_worlds=200):\n",
    "    \"\"\"\n",
    "    Run multiple scan methods (traditional scan statistic and \n",
    "    inside-outside variations) on a set of regions.\n",
    "\n",
    "    This function:\n",
    "        1. Prepares region dictionaries.\n",
    "        2. Computes region sizes and weights.\n",
    "        3. Scans regions using a standard significance test (`get_signif_thresh_scanned_regions`).\n",
    "        4. Scans regions using inside-outside statistics (both original and adjusted weights).\n",
    "        5. Merges results \n",
    "        6. Returns a merged DataFrame of all scan results and a DataFrame of comparison scores.\n",
    "\n",
    "    Args:\n",
    "        y_pred (array-like): Binary predictions (0 or 1) for each point.\n",
    "        points_per_region (list of lists): A list where each element is a list of \n",
    "            point indices corresponding to a region.\n",
    "        signif_level (float, optional): Significance level for thresholds. Defaults to 0.005.\n",
    "        n_alt_worlds (int, optional): Number of alternative worlds to generate. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - all_scanned_regs_info (pd.DataFrame): Merged scan results.\n",
    "            - scores_df (pd.DataFrame): Comparison of methods (TP, FP, TN, FN, total significant).\n",
    "    \"\"\"\n",
    "    regions = [{\"points\": pts} for pts in points_per_region]\n",
    "    N = len(y_pred)\n",
    "    P = np.sum(y_pred)\n",
    "    print(f\"N={N}, P={P}, PR={P/N:.3f}\")\n",
    "\n",
    "    n_s = [len(pts) for pts in points_per_region]\n",
    "    n_out_s = [N - n for n in n_s]\n",
    "    promis_app_weights = [\n",
    "        np.sqrt(n * n_out) / (n + n_out) for n, n_out in zip(n_s, n_out_s)\n",
    "    ]\n",
    "\n",
    "    df_scanned_regs = get_signif_thresh_scanned_regions(\n",
    "        signif_level, n_alt_worlds, regions, y_pred, N, P, seed=seed\n",
    "    )\n",
    "    total_signif_regs = len(df_scanned_regs[df_scanned_regs[\"signif\"] == True])\n",
    "\n",
    "    promis_app_scanned_regs_df = spatial_promis_app_scan_statistic(\n",
    "        points_per_region,\n",
    "        y_pred,\n",
    "        n_alt_worlds=n_alt_worlds,\n",
    "        signif_level=signif_level,\n",
    "        weights=promis_app_weights,\n",
    "        seed=seed,\n",
    "        label=\"promis_app\",\n",
    "    )\n",
    "    total_promis_app_signif_regs = len(\n",
    "        promis_app_scanned_regs_df[promis_app_scanned_regs_df[\"promis_app_signif\"] == True]\n",
    "    )\n",
    "\n",
    "    all_scanned_regs_info = pd.merge(\n",
    "        df_scanned_regs,\n",
    "        promis_app_scanned_regs_df,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    true_signif = df_scanned_regs[\"signif\"].values\n",
    "    promis_app_pred_signif = promis_app_scanned_regs_df[\"promis_app_signif\"].values\n",
    "\n",
    "    promis_app_scores = get_scores(true_signif, promis_app_pred_signif)\n",
    "\n",
    "    scores_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Method\": [\n",
    "                \"Scan Statistics\",\n",
    "                \"Promis App Statistics\",\n",
    "            ],\n",
    "            \"TP\": [None, promis_app_scores[0]],\n",
    "            \"FP\": [None, promis_app_scores[1]],\n",
    "            \"TN\": [None, promis_app_scores[2]],\n",
    "            \"FN\": [None, promis_app_scores[3]],\n",
    "            \"Total Signif Regions\": [\n",
    "                total_signif_regs,\n",
    "                total_promis_app_signif_regs,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        all_scanned_regs_info,\n",
    "        scores_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for XGB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, over_partioning_type_name, xgb_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"K=10, Radii=4\"\n",
    "scores_st_par_val_df['Classifier']=\"XGBoost\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"K=10, Radii=4\"\n",
    "scores_eq_opp_val_df['Classifier']=\"XGBoost\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Non-Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, non_over_partioning_type_name, xgb_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"Non-Overlapping K=8\"\n",
    "scores_st_par_val_df['Classifier']=\"XGBoost\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"Non-Overlapping K=8\"\n",
    "scores_eq_opp_val_df['Classifier']=\"XGBoost\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Grid with max RowsXColumns: 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, grid_partitioning_type_name, xgb_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_st_par_val_df['Classifier']=\"XGBoost\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_eq_opp_val_df['Classifier']=\"XGBoost\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, over_partioning_type_name, dnn_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"Overlapping K=10, Radii=4\"\n",
    "scores_st_par_val_df['Classifier']=\"DNN\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"Overlapping K=10, Radii=4\"\n",
    "scores_eq_opp_val_df['Classifier']=\"DNN\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Non-Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, non_over_partioning_type_name, dnn_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"Non-Overlapping K=8\"\n",
    "scores_st_par_val_df['Classifier']=\"DNN\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"Non-Overlapping K=8\"\n",
    "scores_eq_opp_val_df['Classifier']=\"DNN\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Grid with max RowsXColumns: 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, grid_partitioning_type_name, dnn_clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "\n",
    "\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "\n",
    "val_pred_df['label'] = y_true_val\n",
    "val_pts_per_region = val_regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par_val,\n",
    "    scores_st_par_val_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred_val,\n",
    "    val_pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par_val.head())\n",
    "scores_st_par_val_df['Dataset']=\"Crime\"\n",
    "scores_st_par_val_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_st_par_val_df['Classifier']=\"DNN\"\n",
    "scores_st_par_val_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_val_df)\n",
    "display(scores_st_par_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_y_true_indices, val_pts_per_region_eq_opp = get_pos_info_regions(\n",
    "    y_true_val, val_pts_per_region\n",
    ")\n",
    "\n",
    "pos_val_y_pred = y_pred_val[val_pos_y_true_indices]\n",
    "\n",
    "(\n",
    "    all_scanned_regs_info_eq_opp_val,\n",
    "    scores_eq_opp_val_df,\n",
    ") = run_scan_methods(\n",
    "    pos_val_y_pred,\n",
    "    val_pts_per_region_eq_opp,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_eq_opp_val.head())\n",
    "scores_eq_opp_val_df['Dataset']=\"Crime\"\n",
    "scores_eq_opp_val_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_eq_opp_val_df['Classifier']=\"DNN\"\n",
    "scores_eq_opp_val_df['Scan Type']=\"Equal Opportunity\"\n",
    "all_scores.append(scores_eq_opp_val_df)\n",
    "display(scores_eq_opp_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for LAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    lar_dataset_name, over_partioning_type_name_lar, \"\"\n",
    ")\n",
    "train_path_info, _, _ = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, lar_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(train_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(f\"{base_path}preprocess/lar.csv\")\n",
    "y_pred = get_y(pred_df, \"label\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"LAR\"\n",
    "scores_st_par_df['Partitioning Type']=\"Overlapping K=100, Radii=30\"\n",
    "scores_st_par_df['Classifier']=\"-\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Non-Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    lar_dataset_name, non_over_partioning_type_name_lar, \"\"\n",
    ")\n",
    "train_path_info, _, _ = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, lar_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(train_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(f\"{base_path}preprocess/lar.csv\")\n",
    "y_pred = get_y(pred_df, \"label\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"LAR\"\n",
    "scores_st_par_df['Partitioning Type']=\"Non-Overlapping K=100\"\n",
    "scores_st_par_df['Classifier']=\"-\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Grid with max RowsXColumns: 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    lar_dataset_name, grid_partitioning_type_name, \"\"\n",
    ")\n",
    "train_path_info, _, _ = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, lar_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(train_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(f\"{base_path}preprocess/lar.csv\")\n",
    "y_pred = get_y(pred_df, \"label\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"LAR\"\n",
    "scores_st_par_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_st_par_df['Classifier']=\"-\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for Semi Synthetic: Crime Coordinates, Unfair By Design Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, over_partioning_type_name, f\"{unfair_clf_name}_regions_{over_partioning_type_name}\"\n",
    ")\n",
    "_, _, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "y_pred = get_y(pred_df, \"pred\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"Semi-Synthetic\"\n",
    "scores_st_par_df['Partitioning Type']=\"Overlapping K=10, Radii=4\"\n",
    "scores_st_par_df['Classifier']=\"Unfair by Design\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Non-Overlapping Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, non_over_partioning_type_name, f\"{unfair_clf_name}_regions_{non_over_partioning_type_name}\"\n",
    ")\n",
    "_, _, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "y_pred = get_y(pred_df, \"pred\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"Semi-Synthetic\"\n",
    "scores_st_par_df['Partitioning Type']=\"Non-Overlapping K=10\"\n",
    "scores_st_par_df['Classifier']=\"Unfair by Design\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Detection Method On Grid with max RowsXColumns: 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    crime_dataset_name, grid_partitioning_type_name, f\"{unfair_clf_name}_regions_{grid_partitioning_type_name}\"\n",
    ")\n",
    "_, _, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, crime_dataset_name\n",
    ")\n",
    "\n",
    "regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "y_pred = get_y(pred_df, \"pred\")\n",
    "pts_per_region = regions_df['points'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Testing Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_scanned_regs_info_st_par,\n",
    "    scores_st_par_df,\n",
    ") = run_scan_methods(\n",
    "    y_pred,\n",
    "    pts_per_region,\n",
    "    signif_level=signif_level,\n",
    "    n_alt_worlds=n_alt_worlds,\n",
    ")\n",
    "\n",
    "display(all_scanned_regs_info_st_par.head())\n",
    "scores_st_par_df['Dataset']=\"Semi-Synthetic\"\n",
    "scores_st_par_df['Partitioning Type']=\"Max 5x5 Grid\"\n",
    "scores_st_par_df['Classifier']=\"Unfair by Design\"\n",
    "scores_st_par_df['Scan Type']=\"Statistical Parity\"\n",
    "all_scores.append(scores_st_par_df)\n",
    "display(scores_st_par_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df = pd.concat(all_scores, ignore_index=True)\n",
    "# all_scores_df.to_csv(f\"../../results/audit_scores.csv\", index=False)\n",
    "display(all_scores_df)\n",
    "approx_scores_df = all_scores_df.dropna(subset=[\"TP\", \"FP\", \"TN\", \"FN\"]).astype({\"TP\": float, \"FP\": float, \"TN\": float, \"FN\": float})\n",
    "display(approx_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_scores(TP, FP, TN, FN):\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_total = approx_scores_df[\"TP\"].sum()\n",
    "FP_total = approx_scores_df[\"FP\"].sum()\n",
    "TN_total = approx_scores_df[\"TN\"].sum()\n",
    "FN_total = approx_scores_df[\"FN\"].sum()\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_final_scores(TP_total, FP_total, TN_total, FN_total)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_scores_df[[\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]] = approx_scores_df.apply(\n",
    "    lambda row: pd.Series(get_final_scores(row[\"TP\"], row[\"FP\"], row[\"TN\"], row[\"FN\"])), axis=1\n",
    ")\n",
    "approx_scores_df=approx_scores_df[['Dataset', 'Partitioning Type', 'Classifier', 'Scan Type', 'Total Signif Regions', 'Accuracy', 'Precision', 'Recall', 'F1']]\n",
    "# approx_scores_df.to_csv(f\"../../results/audit_scores_approx.csv\", index=False)\n",
    "display(approx_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_scores_df[['Dataset', 'Partitioning Type', 'Classifier', 'Scan Type', 'Total Signif Regions']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMIS Env",
   "language": "python",
   "name": "promis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
